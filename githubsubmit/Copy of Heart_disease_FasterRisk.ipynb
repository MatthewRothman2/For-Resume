{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"VSe6BQztDWP-","outputId":"7d663903-c729-4b3c-a88b-36a1fc615293"},"outputs":[{"name":"stdout","output_type":"stream","text":["['/home/msr216/.conda/envs/fasterrisk/lib/python39.zip', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9/lib-dynload', '', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9/site-packages']\n","Requirement already satisfied: fasterrisk in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (0.1.10)\n","Requirement already satisfied: matplotlib==3.7.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (3.7.2)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.3 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.26.4)\n","Requirement already satisfied: pandas==1.5.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.5.2)\n","Requirement already satisfied: pillow==9.4.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (9.4.0)\n","Requirement already satisfied: requests<3.0.0,>=2.28.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (2.32.3)\n","Requirement already satisfied: scikit-learn==1.2.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (24.1)\n","Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (2.9.0.post0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (6.4.5)\n","Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from pandas==1.5.2->fasterrisk) (2024.2)\n","Requirement already satisfied: scipy>=1.3.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.13.1)\n","Requirement already satisfied: joblib>=1.1.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (3.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2024.8.30)\n","Requirement already satisfied: zipp>=3.1.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.2->fasterrisk) (3.20.2)\n","Requirement already satisfied: six>=1.5 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->fasterrisk) (1.16.0)\n"]}],"source":["import sys\n","print(sys.path)\n","\n","\n","!pip install fasterrisk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5u_i75G_DWP_"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import roc_auc_score, mean_squared_error, confusion_matrix, accuracy_score\n","from matplotlib import pyplot as plt\n","from sklearn.utils import resample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dtvp0h90DWQA","outputId":"ca41fc80-ea9c-49d6-a0fe-341d8abde11d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of unique values in the last column: 2\n","Number of unique values in the last column: 2\n"]}],"source":["df = pd.read_csv(\"output_train.csv\")\n","unique_count = df.iloc[:, -1].nunique()\n","print(f\"Number of unique values in the last column: {unique_count}\")\n","\n","df = pd.read_csv(\"output_test.csv\")\n","unique_count = df.iloc[:, -1].nunique()\n","print(f\"Number of unique values in the last column: {unique_count}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpbXVxr8DWQA","outputId":"b947cc27-2c19-4a34-e9e1-4414337be2fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        1      0      0      1         1         0       1       0      0   \n","1        1      0      0      1         0         1       0       1      0   \n","2        1      0      0      1         0         1       0       1      1   \n","3        1      0      1      0         0         1       1       0      1   \n","4        1      0      0      1         1         0       1       0      1   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","146      1      0      0      1         0         1       1       0      1   \n","147      1      0      0      1         1         0       1       0      1   \n","148      0      1      0      1         0         1       1       0      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       1       0      1   \n","\n","     fbs_1  ...  caa_3_1  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        1  ...        0        1          1          0          1          0   \n","1        1  ...        0        1          1          0          1          0   \n","2        0  ...        0        1          1          0          1          0   \n","3        0  ...        0        1          1          0          1          0   \n","4        0  ...        0        1          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","146      0  ...        0        1          1          0          1          0   \n","147      0  ...        0        1          1          0          1          0   \n","148      1  ...        0        1          1          0          1          0   \n","149      0  ...        0        1          1          0          1          0   \n","150      0  ...        0        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            0          1          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            0          1          1          0  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","146          1          0          0          1  \n","147          1          0          0          1  \n","148          0          1          1          0  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","\n","[151 rows x 53 columns]\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        1     -1     -1      1         1        -1       1      -1     -1   \n","1        1     -1     -1      1        -1         1      -1       1     -1   \n","2        1     -1     -1      1        -1         1      -1       1      1   \n","3        1     -1      1     -1        -1         1       1      -1      1   \n","4        1     -1     -1      1         1        -1       1      -1      1   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","146      1     -1     -1      1        -1         1       1      -1      1   \n","147      1     -1     -1      1         1        -1       1      -1      1   \n","148     -1      1     -1      1        -1         1       1      -1     -1   \n","149      1     -1     -1      1         1        -1      -1       1      1   \n","150     -1      1      1     -1        -1         1       1      -1      1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        1  ...        1          1         -1          1         -1   \n","1        1  ...        1          1         -1          1         -1   \n","2       -1  ...        1          1         -1          1         -1   \n","3       -1  ...        1          1         -1          1         -1   \n","4       -1  ...        1          1         -1          1         -1   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","146     -1  ...        1          1         -1          1         -1   \n","147     -1  ...        1          1         -1          1         -1   \n","148      1  ...        1          1         -1          1         -1   \n","149     -1  ...        1          1         -1          1         -1   \n","150     -1  ...        1          1         -1          1         -1   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0           -1          1          1         -1       1  \n","1            1         -1         -1          1       1  \n","2            1         -1         -1          1      -1  \n","3           -1          1          1         -1       1  \n","4           -1          1          1         -1       1  \n","..         ...        ...        ...        ...     ...  \n","146          1         -1         -1          1      -1  \n","147          1         -1         -1          1       1  \n","148         -1          1          1         -1       1  \n","149          1         -1         -1          1      -1  \n","150         -1          1          1         -1       1  \n","\n","[151 rows x 54 columns]\n"]}],"source":["df = pd.read_csv(\"output_train.csv\")\n","output = df['output']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","print(df)\n","df = pd.concat([df,output],axis=1)\n","df.replace(df.iloc[:,-1].unique()[0],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[1]],-1,inplace=True)\n","h = df.columns\n","print(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hTWpI1eXDWQA","outputId":"781f9cc9-150c-4972-a361-c14067ff2ca5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed Test DataFrame (before concatenation):\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  caa_4_1  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1        0          1          0          0          1   \n","1        0  ...        1        0          1          0          1          0   \n","2        1  ...        1        0          1          0          1          0   \n","3        0  ...        1        0          1          0          1          0   \n","4        1  ...        1        0          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","147      0  ...        1        0          1          0          1          0   \n","148      1  ...        1        0          1          0          1          0   \n","149      0  ...        1        0          1          0          1          0   \n","150      0  ...        1        0          1          0          1          0   \n","151      0  ...        1        0          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            1          0          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            1          0          0          1  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","147          0          1          1          0  \n","148          1          0          0          1  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","151          0          1          1          0  \n","\n","[152 rows x 54 columns]\n","Final Processed Test DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0       -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","1       -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","2       -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","3       -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","4       -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147     -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","148     -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","149     -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","150     -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","151     -1     -1     -1     -1        -1        -1      -1      -1     -1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0       -1  ...       -1         -1         -1         -1         -1   \n","1       -1  ...       -1         -1         -1         -1         -1   \n","2       -1  ...       -1         -1         -1         -1         -1   \n","3       -1  ...       -1         -1         -1         -1         -1   \n","4       -1  ...       -1         -1         -1         -1         -1   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","147     -1  ...       -1         -1         -1         -1         -1   \n","148     -1  ...       -1         -1         -1         -1         -1   \n","149     -1  ...       -1         -1         -1         -1         -1   \n","150     -1  ...       -1         -1         -1         -1         -1   \n","151     -1  ...       -1         -1         -1         -1         -1   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0           -1         -1         -1         -1      -1  \n","1           -1         -1         -1         -1      -1  \n","2           -1         -1         -1         -1      -1  \n","3           -1         -1         -1         -1      -1  \n","4           -1         -1         -1         -1      -1  \n","..         ...        ...        ...        ...     ...  \n","147         -1         -1         -1         -1      -1  \n","148         -1         -1         -1         -1      -1  \n","149         -1         -1         -1         -1      -1  \n","150         -1         -1         -1         -1      -1  \n","151         -1         -1         -1         -1      -1  \n","\n","[152 rows x 54 columns]\n"]}],"source":["import pandas as pd\n","\n","# Load the test dataset\n","df2 = pd.read_csv(\"output_test.csv\")\n","output = df2['output']  # Store the output column\n","df2 = df2.iloc[:, :-1]  # Drop the output column from features\n","\n","# One-hot encoding for the test data\n","df2 = pd.get_dummies(df2, columns=df2.columns)\n","print(\"Processed Test DataFrame (before concatenation):\")\n","print(df2)\n","\n","# Add the output column back to the test dataframe\n","df2['output'] = output\n","\n","# Replace output values with 1 and -1\n","unique_test = df2['output'].unique()\n","if len(unique_test) > 1:\n","    df2.replace(unique_test[0], 1, inplace=True)\n","    df2.replace(unique_test[1], -1, inplace=True)\n","elif len(unique_test) == 1:\n","    df2['output'] = 1 if unique_test[0] == 1 else -1  # Adjust as needed\n","\n","# Ensure the test DataFrame has the same columns as the training DataFrame\n","# Assuming 'h' is defined from the training DataFrame earlier\n","df2 = df2.reindex(columns=h, fill_value=0)\n","\n","# Final output\n","print(\"Final Processed Test DataFrame:\")\n","print(df2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8IdALYKDWQB","outputId":"f979ba68-b680-4299-89d8-01896fd032df"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed Training DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        1      0      0      1         1         0       1       0      0   \n","1        1      0      0      1         0         1       0       1      0   \n","2        1      0      0      1         0         1       0       1      1   \n","3        1      0      1      0         0         1       1       0      1   \n","4        1      0      0      1         1         0       1       0      1   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","146      1      0      0      1         0         1       1       0      1   \n","147      1      0      0      1         1         0       1       0      1   \n","148      0      1      0      1         0         1       1       0      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       1       0      1   \n","\n","     fbs_1  ...  caa_3_1  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        1  ...        0        1          1          0          1          0   \n","1        1  ...        0        1          1          0          1          0   \n","2        0  ...        0        1          1          0          1          0   \n","3        0  ...        0        1          1          0          1          0   \n","4        0  ...        0        1          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","146      0  ...        0        1          1          0          1          0   \n","147      0  ...        0        1          1          0          1          0   \n","148      1  ...        0        1          1          0          1          0   \n","149      0  ...        0        1          1          0          1          0   \n","150      0  ...        0        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            0          1          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            0          1          1          0  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","146          1          0          0          1  \n","147          1          0          0          1  \n","148          0          1          1          0  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","\n","[151 rows x 53 columns]\n","\n","Final Processed Training DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        1      0      0      1         1         0       1       0      0   \n","1        1      0      0      1         0         1       0       1      0   \n","2        1      0      0      1         0         1       0       1      1   \n","3        1      0      1      0         0         1       1       0      1   \n","4        1      0      0      1         1         0       1       0      1   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","146      1      0      0      1         0         1       1       0      1   \n","147      1      0      0      1         1         0       1       0      1   \n","148      0      1      0      1         0         1       1       0      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        1  ...        1          1          0          1          0   \n","1        1  ...        1          1          0          1          0   \n","2        0  ...        1          1          0          1          0   \n","3        0  ...        1          1          0          1          0   \n","4        0  ...        1          1          0          1          0   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","146      0  ...        1          1          0          1          0   \n","147      0  ...        1          1          0          1          0   \n","148      1  ...        1          1          0          1          0   \n","149      0  ...        1          1          0          1          0   \n","150      0  ...        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0            0          1          1          0       1  \n","1            1          0          0          1       1  \n","2            1          0          0          1      -1  \n","3            0          1          1          0       1  \n","4            0          1          1          0       1  \n","..         ...        ...        ...        ...     ...  \n","146          1          0          0          1      -1  \n","147          1          0          0          1       1  \n","148          0          1          1          0       1  \n","149          1          0          0          1      -1  \n","150          0          1          1          0       1  \n","\n","[151 rows x 54 columns]\n","\n","Processed training data has been saved to 'processed_train_data.csv'.\n","\n","Processed Test DataFrame (before concatenation):\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  caa_4_1  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1        0          1          0          0          1   \n","1        0  ...        1        0          1          0          1          0   \n","2        1  ...        1        0          1          0          1          0   \n","3        0  ...        1        0          1          0          1          0   \n","4        1  ...        1        0          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","147      0  ...        1        0          1          0          1          0   \n","148      1  ...        1        0          1          0          1          0   \n","149      0  ...        1        0          1          0          1          0   \n","150      0  ...        1        0          1          0          1          0   \n","151      0  ...        1        0          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            1          0          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            1          0          0          1  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","147          0          1          1          0  \n","148          1          0          0          1  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","151          0          1          1          0  \n","\n","[152 rows x 54 columns]\n","\n","Final Processed Test DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1          1          0          0          1   \n","1        0  ...        1          1          0          1          0   \n","2        1  ...        1          1          0          1          0   \n","3        0  ...        1          1          0          1          0   \n","4        1  ...        1          1          0          1          0   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","147      0  ...        1          1          0          1          0   \n","148      1  ...        1          1          0          1          0   \n","149      0  ...        1          1          0          1          0   \n","150      0  ...        1          1          0          1          0   \n","151      0  ...        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0            1          0          1          0      -1  \n","1            1          0          0          1      -1  \n","2            1          0          0          1       1  \n","3            1          0          0          1      -1  \n","4            0          1          1          0       1  \n","..         ...        ...        ...        ...     ...  \n","147          0          1          1          0       1  \n","148          1          0          0          1      -1  \n","149          1          0          0          1      -1  \n","150          0          1          1          0       1  \n","151          0          1          1          0       1  \n","\n","[152 rows x 54 columns]\n","\n","Processed test data has been saved to 'processed_test_data.csv'.\n"]}],"source":["import pandas as pd\n","\n","# Load the training dataset\n","df = pd.read_csv(\"output_train.csv\")\n","output = df['output']\n","df = df.iloc[:, :-1]  # Drop the output column from features\n","\n","# One-hot encoding for the training data\n","df = pd.get_dummies(df, columns=df.columns)\n","print(\"Processed Training DataFrame:\")\n","print(df)\n","\n","# Add the output column back to the training DataFrame\n","df['output'] = output\n","\n","# Replace output values (0 -> -1, 1 -> 1)\n","df['output'].replace(0, -1, inplace=True)\n","df['output'].replace(1, 1, inplace=True)\n","\n","# Store the column names for later use\n","h = df.columns\n","\n","# Print final processed training DataFrame\n","print(\"\\nFinal Processed Training DataFrame:\")\n","print(df)\n","\n","# Save the processed training DataFrame to a CSV file\n","df.to_csv(\"processed_train_data.csv\", index=False)\n","print(\"\\nProcessed training data has been saved to 'processed_train_data.csv'.\")\n","\n","# Load the test dataset\n","df2 = pd.read_csv(\"output_test.csv\")\n","output2 = df2['output']\n","df2 = df2.iloc[:, :-1]  # Drop the output column from features\n","\n","# One-hot encoding for the test data\n","df2 = pd.get_dummies(df2, columns=df2.columns)\n","print(\"\\nProcessed Test DataFrame (before concatenation):\")\n","print(df2)\n","\n","# Add the output column back to the test DataFrame\n","df2['output'] = output2\n","\n","# Replace output values (0 -> -1, 1 -> 1)\n","df2['output'].replace(0, -1, inplace=True)\n","df2['output'].replace(1, 1, inplace=True)\n","\n","# Ensure the test DataFrame has the same columns as the training DataFrame\n","df2 = df2.reindex(columns=h, fill_value=0)\n","\n","# Print final processed test DataFrame\n","print(\"\\nFinal Processed Test DataFrame:\")\n","print(df2)\n","\n","# Save the processed test DataFrame to a CSV file\n","df2.to_csv(\"processed_test_data.csv\", index=False)\n","print(\"\\nProcessed test data has been saved to 'processed_test_data.csv'.\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0trlg2RDWQB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrNFiorUDWQB"},"outputs":[],"source":["X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['output'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['output'].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtfcIUT4DWQB","outputId":"51407b81-07ee-4a03-97e2-aa71fd1fccea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8874172185430463\n","AUC:  0.9546420978029766\n","Precision:  0.8898596640590654\n","Recall:  0.8874172185430463\n","Total Run Time: 4.9614 seconds\n","Risk Score Model Coefficients:\n","Multiplier:  1.0\n","Intercept:  -5.0\n","Coefficients:  [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0. -1.  0.  0.  1.  0.\n","  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0. -5.  3.  0.  2.  0.  0.  0.\n"," -5.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.]\n"]}],"source":["import time\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","from sklearn.metrics import precision_score, recall_score, roc_auc_score\n","\n","# Start the total runtime timer\n","start_time = time.time()\n","\n","sparsity = 10  # produce a risk score model with 10 nonzero coefficients\n","\n","# Initialize a risk score optimizer\n","m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n","\n","# Perform optimization\n","m.optimize()\n","\n","# Get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models()\n","\n","# Get the first solution from the final diverse pool\n","multiplier, intercept, coefficients = m.get_models(model_index=0)\n","\n","# Feature names\n","X_featureNames = df.iloc[:, :-1].columns.values.tolist()\n","\n","# Create a classifier\n","clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n","\n","# Get the predicted label\n","y_pred = clf.predict(X=X_train)\n","\n","# Get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X=X_train)\n","\n","# Compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X=X_train, y=y_train)\n","\n","# Get accuracy and area under the ROC curve (AUC)\n","acc_train, auc_train = clf.get_acc_and_auc(X=X_train, y=y_train)\n","\n","# Calculate precision and recall\n","precision_train = precision_score(y_train, y_pred, average='weighted')\n","recall_train = recall_score(y_train, y_pred, average='weighted')\n","\n","# End the total runtime timer\n","end_time = time.time()\n","total_run_time = end_time - start_time\n","\n","# Print the results\n","print(\"Accuracy: \", acc_train)\n","print(\"AUC: \", auc_train)\n","print(\"Precision: \", precision_train)\n","print(\"Recall: \", recall_train)\n","\n","# Print the total runtime\n","print(f\"Total Run Time: {total_run_time:.4f} seconds\")\n","\n","# Optionally, you can print the risk score model information:\n","print(\"Risk Score Model Coefficients:\")\n","print(\"Multiplier: \", multiplier)\n","print(\"Intercept: \", intercept)\n","print(\"Coefficients: \", coefficients)\n","\n","# Print the risk s"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Km4ziuzMDWQC","outputId":"eecdfbcc-b63c-4d8a-d8c4-0129dd233f7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Testing Metrics:\n","Accuracy:  0.8092105263157895\n","AUC:  0.8812717770034842\n","Precision:  0.8115402141717931\n","Recall:  0.8092105263157895\n","\n","Confusion Matrix (Test):\n","[[58 12]\n"," [17 65]]\n","The Risk Score is:\n","1.      chol_0      1 point(s) |   ...\n","2.      exng_1     -1 point(s) | + ...\n","3.      cp_0_0      1 point(s) | + ...\n","4.      cp_2_1      2 point(s) | + ...\n","5. restecg_2_1     -5 point(s) | + ...\n","6.     slp_0_0      3 point(s) | + ...\n","7.     slp_1_0      2 point(s) | + ...\n","8.     caa_0_0     -5 point(s) | + ...\n","9.     caa_1_1      1 point(s) | + ...\n","10.   thall_3_0      3 point(s) | + ...\n","                         SCORE | =    \n","SCORE |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.7% |   1.8% |\n","SCORE |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |\n","RISK  |   4.7% |  11.9% |  26.9% |  50.0% |  73.1% |  88.1% |  95.3% |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% |\n"]}],"source":["from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n","import numpy as np\n","\n","# Sparsity: Number of non-zero coefficients in the model\n","sparsity = 10  # Produce a risk score model with 10 nonzero coefficients\n","\n","# Initialize a risk score optimizer with training data\n","m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n","\n","# Perform optimization\n","m.optimize()\n","\n","# Get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models()\n","\n","# Get the first solution from the final diverse pool\n","multiplier, intercept, coefficients = m.get_models(model_index=0)\n","\n","# Feature names\n","X_featureNames = df2.iloc[:, :-1].columns.values.tolist()  # Assuming df2 has the same structure\n","\n","# Create a classifier\n","clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n","\n","# Get the predicted label for the test set\n","y_test_pred = clf.predict(X=X_test)\n","\n","# Get the probability of predicting y[i] with label +1 for the test set\n","try:\n","    y_test_pred_prob = clf.predict_prob(X=X_test)\n","except Exception as e:\n","    print(\"Error while predicting probabilities:\", e)\n","    y_test_pred_prob = (intercept + X_test.dot(coefficients)) / multiplier\n","\n","# Compute the logistic loss for the test set\n","logisticLoss_test = clf.compute_logisticLoss(X=X_test, y=y_test)\n","\n","# Get accuracy and area under the ROC curve (AUC) for the test set\n","acc_test, auc_test = clf.get_acc_and_auc(X=X_test, y=y_test)\n","\n","# Calculate precision and recall for the test set\n","precision_test = precision_score(y_test, y_test_pred, average='weighted')\n","recall_test = recall_score(y_test, y_test_pred, average='weighted')\n","\n","# Print the test metrics\n","print(\"\\nTesting Metrics:\")\n","print(\"Accuracy: \", acc_test)\n","print(\"AUC: \", auc_test)\n","print(\"Precision: \", precision_test)\n","print(\"Recall: \", recall_test)\n","\n","# Calculate confusion matrix for the test set\n","cm_test = confusion_matrix(y_test, y_test_pred)\n","\n","# Print the confusion matrix\n","print(\"\\nConfusion Matrix (Test):\")\n","print(cm_test)\n","\n","# Print the risk score model card\n","clf.print_model_card()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CL2DMyH_DWQC","outputId":"ec13f73d-51cb-48da-92c1-5113f4952014"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8874172185430463\n","AUC:  0.9546420978029766\n","Precision:  0.8898596640590654\n","Recall:  0.8874172185430463\n"]}],"source":["print(\"Accuracy: \", acc_train)\n","print(\"AUC: \", auc_train)\n","print(\"Precision: \", precision_train)\n","print(\"Recall: \", recall_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2tnsq21DWQC","outputId":"8c223797-f55f-458c-dc5e-2c11dc0336af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy:  0.8092105263157895 \n","AUC:  0.8812717770034842\n"]}],"source":["acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbRFq328DWQC","outputId":"3f76bbfb-02a8-48c3-d32a-dd09d491aa1f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Loss:  40.210137983315626\n","Test Loss:  81.58586681942867\n"]}],"source":["logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OG-rBl4YDWQD","outputId":"f50151cb-02a2-4bf6-fb42-7f9130f23034"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed Test DataFrame (before concatenation):\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  caa_4_1  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1        0          1          0          0          1   \n","1        0  ...        1        0          1          0          1          0   \n","2        1  ...        1        0          1          0          1          0   \n","3        0  ...        1        0          1          0          1          0   \n","4        1  ...        1        0          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","147      0  ...        1        0          1          0          1          0   \n","148      1  ...        1        0          1          0          1          0   \n","149      0  ...        1        0          1          0          1          0   \n","150      0  ...        1        0          1          0          1          0   \n","151      0  ...        1        0          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            1          0          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            1          0          0          1  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","147          0          1          1          0  \n","148          1          0          0          1  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","151          0          1          1          0  \n","\n","[152 rows x 54 columns]\n","Unique values in output before mapping: [0 1]\n","Output column after mapping:\n","[ 1 -1]\n","Final Processed Test DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1          1          0          0          1   \n","1        0  ...        1          1          0          1          0   \n","2        1  ...        1          1          0          1          0   \n","3        0  ...        1          1          0          1          0   \n","4        1  ...        1          1          0          1          0   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","147      0  ...        1          1          0          1          0   \n","148      1  ...        1          1          0          1          0   \n","149      0  ...        1          1          0          1          0   \n","150      0  ...        1          1          0          1          0   \n","151      0  ...        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0            1          0          1          0       1  \n","1            1          0          0          1       1  \n","2            1          0          0          1      -1  \n","3            1          0          0          1       1  \n","4            0          1          1          0      -1  \n","..         ...        ...        ...        ...     ...  \n","147          0          1          1          0      -1  \n","148          1          0          0          1       1  \n","149          1          0          0          1       1  \n","150          0          1          1          0      -1  \n","151          0          1          1          0      -1  \n","\n","[152 rows x 54 columns]\n","Testing Metrics:\n","Accuracy:  0.19078947368421054\n","AUC:  0.1187282229965157\n","Precision:  0.15584415584415584\n","Recall:  0.17142857142857143\n","\n","Confusion Matrix (Test):\n","[[17 65]\n"," [58 12]]\n","The Risk Score is:\n","1.      chol_0      1 point(s) |   ...\n","2.      exng_1     -1 point(s) | + ...\n","3.      cp_0_0      1 point(s) | + ...\n","4.      cp_2_1      2 point(s) | + ...\n","5. restecg_2_1     -5 point(s) | + ...\n","6.     slp_0_0      3 point(s) | + ...\n","7.     slp_1_0      2 point(s) | + ...\n","8.     caa_0_0     -5 point(s) | + ...\n","9.     caa_1_1      1 point(s) | + ...\n","10.   thall_3_0      3 point(s) | + ...\n","                         SCORE | =    \n","SCORE |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.7% |   1.8% |\n","SCORE |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |\n","RISK  |   4.7% |  11.9% |  26.9% |  50.0% |  73.1% |  88.1% |  95.3% |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% |\n"]}],"source":["import pandas as pd\n","from sklearn.metrics import precision_score, recall_score, confusion_matrix\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","# Load the test dataset\n","df2 = pd.read_csv(\"output_test.csv\")\n","output = df2['output']  # Store the output column\n","df2 = df2.iloc[:, :-1]  # Drop the output column from features\n","\n","# One-hot encoding for the test data\n","df2 = pd.get_dummies(df2, columns=df2.columns)\n","print(\"Processed Test DataFrame (before concatenation):\")\n","print(df2)\n","\n","# Add the output column back to the test dataframe\n","df2['output'] = output\n","\n","# Check unique values in the output column before replacement\n","unique_test = df2['output'].unique()\n","print(\"Unique values in output before mapping:\", unique_test)\n","\n","# Replace output values with 1 and -1 using mapping\n","if len(unique_test) > 1:\n","    df2['output'] = df2['output'].map({unique_test[0]: 1, unique_test[1]: -1})\n","elif len(unique_test) == 1:\n","    df2['output'] = 1 if unique_test[0] == 1 else -1  # Adjust as needed\n","\n","# Check if mapping was successful\n","print(\"Output column after mapping:\")\n","print(df2['output'].unique())\n","\n","# Ensure the test DataFrame has the same columns as the training DataFrame\n","# Assuming 'h' is defined from the training DataFrame earlier\n","df2 = df2.reindex(columns=h, fill_value=0)\n","\n","# Final output\n","print(\"Final Processed Test DataFrame:\")\n","print(df2)\n","\n","# Prepare the test data\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['output'].to_numpy()\n","\n","# Use the pre-trained model from earlier (assuming it's already been trained with your training data)\n","# Initialize the model with appropriate sparsity\n","sparsity = 10  # produce a risk score model with 10 nonzero coefficients\n","\n","# Initialize a risk score optimizer with the training data (use existing training data for optimization)\n","m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n","\n","# Perform optimization\n","m.optimize()\n","\n","# Get the first solution from the final diverse pool\n","multiplier, intercept, coefficients = m.get_models(model_index=0)\n","\n","# Feature names for training data\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()\n","\n","# Create a classifier using the optimized parameters\n","clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n","\n","# Test evaluation (using the processed test data)\n","y_test_pred = clf.predict(X=X_test)\n","y_test_pred_prob = clf.predict_prob(X=X_test)\n","\n","# Test metrics\n","acc_test, auc_test = clf.get_acc_and_auc(X=X_test, y=y_test)\n","precision_test = precision_score(y_test, y_test_pred, average='binary')  # Binary precision\n","recall_test = recall_score(y_test, y_test_pred, average='binary')  # Binary recall\n","\n","# Print the results for test metrics\n","print(\"Testing Metrics:\")\n","print(\"Accuracy: \", acc_test)\n","print(\"AUC: \", auc_test)\n","print(\"Precision: \", precision_test)\n","print(\"Recall: \", recall_test)\n","\n","# Calculate confusion matrix for the test set\n","cm_test = confusion_matrix(y_test, y_test_pred)\n","\n","# Print the confusion matrix for the test set\n","print(\"\\nConfusion Matrix (Test):\")\n","print(cm_test)\n","\n","# Print the risk score model card\n","clf.print_model_card()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZMolfERDWQD","outputId":"fc074e5b-fcfe-4ad1-d26b-2fe8deca251d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed Test DataFrame (before concatenation):\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  caa_4_1  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1        0          1          0          0          1   \n","1        0  ...        1        0          1          0          1          0   \n","2        1  ...        1        0          1          0          1          0   \n","3        0  ...        1        0          1          0          1          0   \n","4        1  ...        1        0          1          0          1          0   \n","..     ...  ...      ...      ...        ...        ...        ...        ...   \n","147      0  ...        1        0          1          0          1          0   \n","148      1  ...        1        0          1          0          1          0   \n","149      0  ...        1        0          1          0          1          0   \n","150      0  ...        1        0          1          0          1          0   \n","151      0  ...        1        0          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  \n","0            1          0          1          0  \n","1            1          0          0          1  \n","2            1          0          0          1  \n","3            1          0          0          1  \n","4            0          1          1          0  \n","..         ...        ...        ...        ...  \n","147          0          1          1          0  \n","148          1          0          0          1  \n","149          1          0          0          1  \n","150          0          1          1          0  \n","151          0          1          1          0  \n","\n","[152 rows x 54 columns]\n","Unique values in output before mapping: [0 1]\n","Output column after mapping:\n","[ 1 -1]\n","Final Processed Test DataFrame:\n","     age_0  age_1  sex_0  sex_1  trtbps_0  trtbps_1  chol_0  chol_1  fbs_0  \\\n","0        0      1      0      1         0         1       0       1      1   \n","1        0      1      0      1         0         1       0       1      1   \n","2        0      1      0      1         0         1       1       0      0   \n","3        0      1      1      0         0         1       0       1      1   \n","4        0      1      1      0         1         0       0       1      0   \n","..     ...    ...    ...    ...       ...       ...     ...     ...    ...   \n","147      0      1      1      0         1         0       0       1      1   \n","148      1      0      1      0         0         1       0       1      0   \n","149      1      0      0      1         1         0       0       1      1   \n","150      0      1      1      0         0         1       0       1      1   \n","151      1      0      1      0         1         0       1       0      1   \n","\n","     fbs_1  ...  caa_4_0  thall_0_0  thall_0_1  thall_1_0  thall_1_1  \\\n","0        0  ...        1          1          0          0          1   \n","1        0  ...        1          1          0          1          0   \n","2        1  ...        1          1          0          1          0   \n","3        0  ...        1          1          0          1          0   \n","4        1  ...        1          1          0          1          0   \n","..     ...  ...      ...        ...        ...        ...        ...   \n","147      0  ...        1          1          0          1          0   \n","148      1  ...        1          1          0          1          0   \n","149      0  ...        1          1          0          1          0   \n","150      0  ...        1          1          0          1          0   \n","151      0  ...        1          1          0          1          0   \n","\n","     thall_2_0  thall_2_1  thall_3_0  thall_3_1  output  \n","0            1          0          1          0       1  \n","1            1          0          0          1       1  \n","2            1          0          0          1      -1  \n","3            1          0          0          1       1  \n","4            0          1          1          0      -1  \n","..         ...        ...        ...        ...     ...  \n","147          0          1          1          0      -1  \n","148          1          0          0          1       1  \n","149          1          0          0          1       1  \n","150          0          1          1          0      -1  \n","151          0          1          1          0      -1  \n","\n","[152 rows x 54 columns]\n","Testing Metrics:\n","Accuracy:  0.19078947368421054\n","AUC:  0.1187282229965157\n","Precision:  0.15584415584415584\n","Recall:  0.17142857142857143\n","\n","Confusion Matrix (Test):\n","[[17 65]\n"," [58 12]]\n","The Risk Score is:\n","1.      chol_0      1 point(s) |   ...\n","2.      exng_1     -1 point(s) | + ...\n","3.      cp_0_0      1 point(s) | + ...\n","4.      cp_2_1      2 point(s) | + ...\n","5. restecg_2_1     -5 point(s) | + ...\n","6.     slp_0_0      3 point(s) | + ...\n","7.     slp_1_0      2 point(s) | + ...\n","8.     caa_0_0     -5 point(s) | + ...\n","9.     caa_1_1      1 point(s) | + ...\n","10.   thall_3_0      3 point(s) | + ...\n","                         SCORE | =    \n","SCORE |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.7% |   1.8% |\n","SCORE |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |\n","RISK  |   4.7% |  11.9% |  26.9% |  50.0% |  73.1% |  88.1% |  95.3% |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% |\n"]}],"source":["import pandas as pd\n","from sklearn.metrics import precision_score, recall_score, confusion_matrix\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","# Load the test dataset\n","df2 = pd.read_csv(\"output_test.csv\")\n","output = df2['output']  # Store the output column\n","df2 = df2.iloc[:, :-1]  # Drop the output column from features\n","\n","# One-hot encoding for the test data\n","df2 = pd.get_dummies(df2, columns=df2.columns)\n","print(\"Processed Test DataFrame (before concatenation):\")\n","print(df2)\n","\n","# Add the output column back to the test dataframe\n","df2['output'] = output\n","\n","# Check unique values in the output column before replacement\n","unique_test = df2['output'].unique()\n","print(\"Unique values in output before mapping:\", unique_test)\n","\n","# Replace output values with 1 and -1 using mapping\n","if len(unique_test) > 1:\n","    df2['output'] = df2['output'].map({unique_test[0]: 1, unique_test[1]: -1})\n","elif len(unique_test) == 1:\n","    df2['output'] = 1 if unique_test[0] == 1 else -1  # Adjust as needed\n","\n","# Check if mapping was successful\n","print(\"Output column after mapping:\")\n","print(df2['output'].unique())\n","\n","# Ensure the test DataFrame has the same columns as the training DataFrame\n","# Assuming 'h' is defined from the training DataFrame earlier\n","df2 = df2.reindex(columns=h, fill_value=0)\n","\n","# Final output\n","print(\"Final Processed Test DataFrame:\")\n","print(df2)\n","\n","# Prepare the test data\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['output'].to_numpy()\n","\n","# Use the pre-trained model from earlier (assuming it's already been trained with your training data)\n","# Initialize the model with appropriate sparsity\n","sparsity = 10  # produce a risk score model with 10 nonzero coefficients\n","\n","# Initialize a risk score optimizer with the training data (use existing training data for optimization)\n","m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n","\n","# Perform optimization\n","m.optimize()\n","\n","# Get the first solution from the final diverse pool\n","multiplier, intercept, coefficients = m.get_models(model_index=0)\n","\n","# Feature names for training data\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()\n","\n","# Create a classifier using the optimized parameters\n","clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n","\n","# Test evaluation (using the processed test data)\n","y_test_pred = clf.predict(X=X_test)\n","y_test_pred_prob = clf.predict_prob(X=X_test)\n","\n","# Test metrics\n","acc_test, auc_test = clf.get_acc_and_auc(X=X_test, y=y_test)\n","precision_test = precision_score(y_test, y_test_pred, average='binary')  # Binary precision\n","recall_test = recall_score(y_test, y_test_pred, average='binary')  # Binary recall\n","\n","# Print the results for test metrics\n","print(\"Testing Metrics:\")\n","print(\"Accuracy: \", acc_test)\n","print(\"AUC: \", auc_test)\n","print(\"Precision: \", precision_test)\n","print(\"Recall: \", recall_test)\n","\n","# Calculate confusion matrix for the test set\n","cm_test = confusion_matrix(y_test, y_test_pred)\n","\n","# Print the confusion matrix for the test set\n","print(\"\\nConfusion Matrix (Test):\")\n","print(cm_test)\n","\n","# Print the risk score model card\n","clf.print_model_card()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MetzslEHDWQD","outputId":"46df09ac-951d-49df-c77f-7005833c322a"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.     RBFOX1_1     -2 point(s) |   ...\n","2.    CDKN2A_-2     -5 point(s) | + ...\n","3.     CDKN2A_0      1 point(s) | + ...\n","4.      TPTE2_0      2 point(s) | + ...\n","5.     ZNF501_2      3 point(s) | + ...\n","6.       NRG2_0      2 point(s) | + ...\n","7.      CDKL3_0      2 point(s) | + ...\n","8.   RARRES1_-1     -2 point(s) | + ...\n","9.    MIR1224_2      5 point(s) | + ...\n","10.       GPHN_1      3 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |   4.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.3% |   0.5% |   0.9% |   1.6% |   2.8% |   5.0% |   8.6% |\n","SCORE |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  17.0  |  18.0  |\n","RISK  |  14.6% |  23.5% |  35.7% |  50.0% |  64.3% |  76.5% |  85.4% |  91.4% |  95.0% |  97.2% |  98.4% |  99.1% |  99.5% |  99.7% |\n","Training Loss:  1534.7162291162604\n","Test Loss:  256.2647676203032\n","Accuracy:  0.9083530106257379 \n","AUC:  0.8764482184965284\n","Accuracy:  0.9231378763866878 \n","AUC:  0.8891101390611739\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[2],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,3,4,5,6,7,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[8],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fr7ktdzwDWQD","outputId":"2e9d2d3b-8bb3-4779-c097-a987c0940abe"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.    RBFOX1_-1     -3 point(s) |   ...\n","2.     RBFOX1_1      3 point(s) | + ...\n","3.    IL17RE_-1     -4 point(s) | + ...\n","4.     GATA3_-1     -2 point(s) | + ...\n","5.       IHO1_0      2 point(s) | + ...\n","6.    MIR4732_2     -5 point(s) | + ...\n","7.      LCOR_-1      4 point(s) | + ...\n","8.     PTF1A_-1      2 point(s) | + ...\n","9.      ITGB1_2      5 point(s) | + ...\n","10.    MEGF10_-1      2 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.2% |   0.3% |   0.4% |   0.7% |   1.1% |   1.7% |   2.6% |   4.0% |   6.2% |\n","SCORE |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  18.0  |\n","RISK  |   9.4% |  14.0% |  20.4% |  28.8% |  38.8% |  50.0% |  61.2% |  71.2% |  79.6% |  86.0% |  90.6% |  93.8% |  96.0% |  97.4% |  98.9% |\n","Training Loss:  1532.0687502892756\n","Test Loss:  446.131283291956\n","Accuracy:  0.91956906729634 \n","AUC:  0.861627255546714\n","Accuracy:  0.8605388272583201 \n","AUC:  0.8628933684145104\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[3],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,4,5,6,7,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[1],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,2,3,4,5,6,7,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5sbrd-ZJDWQD","outputId":"79ccdd38-b1f3-496c-8f27-458ede9ae1c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.    NKIRAS1_0      5 point(s) |   ...\n","2. CDKN2A-DT_-2      3 point(s) | + ...\n","3.   MIR378C_-1      4 point(s) | + ...\n","4.    ERICH6_-1      3 point(s) | + ...\n","5.       SIL1_0      2 point(s) | + ...\n","6.      NICN1_1      5 point(s) | + ...\n","7.     EDEM1_-1     -3 point(s) | + ...\n","8.      APOD_-1      3 point(s) | + ...\n","9.     PLXDC1_1     -5 point(s) | + ...\n","10.         SI_1      3 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -8.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.3% |   0.5% |   1.0% |   2.0% |   3.7% |   6.9% |  12.4% |\n","SCORE |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  17.0  |  18.0  |  19.0  |  20.0  |  21.0  |  22.0  |  23.0  |  24.0  |  25.0  |  26.0  |  28.0  |\n","RISK  |  21.4% |  34.3% |  50.0% |  65.7% |  78.6% |  87.6% |  93.1% |  96.3% |  98.0% |  99.0% |  99.5% |  99.7% |  99.9% |  99.9% | 100.0% | 100.0% | 100.0% |\n","Training Loss:  742.0367316585562\n","Test Loss:  69.91383453145895\n","Accuracy:  0.9654663518299882 \n","AUC:  0.9627408227079385\n","Accuracy:  0.9857369255150554 \n","AUC:  0.9700162074554295\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[4],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,5,6,7,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[2],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,3,4,5,6,7,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6H_Ayj8jDWQE","outputId":"c670b598-ffd4-4c8a-9355-5393a27c555f"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.    YTHDC2_-1      2 point(s) |   ...\n","2.     NEK10_-1      2 point(s) | + ...\n","3.     SRCIN1_2     -5 point(s) | + ...\n","4.     METTL6_1     -4 point(s) | + ...\n","5.       MBL2_1      2 point(s) | + ...\n","6.    PCDHB12_2      4 point(s) | + ...\n","7.    POU4F3_-1      1 point(s) | + ...\n","8.   PCDHGA5_-2      5 point(s) | + ...\n","9.   SNORD69_-1      1 point(s) | + ...\n","10.     ACSL5_-1     -2 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.3% |   0.5% |   0.9% |   1.6% |   2.8% |   5.0% |\n","SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  17.0  |\n","RISK  |   8.6% |  14.6% |  23.5% |  35.7% |  50.0% |  64.3% |  76.5% |  85.4% |  91.4% |  95.0% |  97.2% |  98.4% |  99.1% |  99.5% |\n","Training Loss:  1268.189997426982\n","Test Loss:  361.46888821878275\n","Accuracy:  0.9303423848878394 \n","AUC:  0.9118633922521471\n","Accuracy:  0.8716323296354992 \n","AUC:  0.9002178796046719\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[5],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,6,7,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[6],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,7,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZuirExODWQE","outputId":"453cd9d8-e0b0-4201-8957-46c299307a07"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.     NLGN1_-1     -4 point(s) |   ...\n","2.    RHBDL3_-1      2 point(s) | + ...\n","3.     IQCF2_-1     -2 point(s) | + ...\n","4.    THNSL1_-1     -3 point(s) | + ...\n","5.       PURA_0     -3 point(s) | + ...\n","6.   RPL22L1_-2      5 point(s) | + ...\n","7.   RPL22L1_-1      3 point(s) | + ...\n","8.     PHF12_-1      3 point(s) | + ...\n","9.   SLC12A2_-1      3 point(s) | + ...\n","10.      CHSY3_0     -2 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.2% |   0.4% |   0.7% |   1.3% |   2.4% |   4.3% |\n","SCORE |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  16.0  |\n","RISK  |   7.8% |  13.5% |  22.5% |  35.0% |  50.0% |  65.0% |  77.5% |  86.5% |  92.2% |  95.7% |  97.6% |  98.7% |  99.3% |  99.8% |\n","Training Loss:  915.759501656592\n","Test Loss:  99.58753425542326\n","Accuracy:  0.9445100354191264 \n","AUC:  0.9612498039030747\n","Accuracy:  0.9778129952456418 \n","AUC:  0.9713892961876833\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[6],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,7,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[7],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,6,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GFA0OLVsDWQE","outputId":"af79b342-af7c-4f5d-8e66-fc7f626a9219"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1. CDKN2A-DT_-1     -1 point(s) |   ...\n","2.    RBFOX1_-2      4 point(s) | + ...\n","3.    CDKN2A_-2     -4 point(s) | + ...\n","4.    AKR1C6P_0      2 point(s) | + ...\n","5.      ALG3_-1     -2 point(s) | + ...\n","6.      NFAT5_1      3 point(s) | + ...\n","7.      NFAT5_2      5 point(s) | + ...\n","8.      LCOR_-1     -2 point(s) | + ...\n","9.     EDEM1_-1      2 point(s) | + ...\n","10.    PTPN23_-1     -1 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.7% |   1.8% |   4.7% |  11.9% |  26.9% |\n","SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |\n","RISK  |  50.0% |  73.1% |  88.1% |  95.3% |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% | 100.0% | 100.0% | 100.0% | 100.0% |\n","Training Loss:  1081.9888367568756\n","Test Loss:  271.1745255382249\n","Accuracy:  0.9446576151121606 \n","AUC:  0.9361813526892896\n","Accuracy:  0.919175911251981 \n","AUC:  0.9213019823313942\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[7],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,8,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[9],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,8]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CTxOF3AZDWQE","outputId":"a7e1cab0-4485-49ca-c3d0-b2d47c1fe4ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.       PIGW_1      2 point(s) |   ...\n","2.      AGTR1_1     -2 point(s) | + ...\n","3.     PRKCQ_-1     -4 point(s) | + ...\n","4.  B3GALNT1_-1      3 point(s) | + ...\n","5.      ALG3_-2      4 point(s) | + ...\n","6.      ALG3_-1      3 point(s) | + ...\n","7.       TNS4_2     -4 point(s) | + ...\n","8.     CHSY3_-1     -2 point(s) | + ...\n","9.        SI_-1      3 point(s) | + ...\n","10.     TPGS1_-1     -2 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.1% |   0.2% |   0.4% |   0.6% |   0.9% |   1.5% |   2.4% |   3.7% |   5.8% |\n","SCORE |   1.0  |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  15.0  |\n","RISK  |   8.9% |  13.4% |  19.8% |  28.3% |  38.6% |  50.0% |  61.4% |  71.7% |  80.2% |  86.6% |  91.1% |  94.2% |  96.3% |  98.5% |\n","Training Loss:  1674.2931872335616\n","Test Loss:  381.0453722938337\n","Accuracy:  0.91086186540732 \n","AUC:  0.8263336250499111\n","Accuracy:  0.9057052297939778 \n","AUC:  0.6925313776999417\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[8],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,9]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[5],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,6,7,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZ5hVGxNDWQE","outputId":"287e55e6-828a-4929-d14a-25d53622cc8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Risk Score is:\n","1.    CDKN2A_-2     -2 point(s) |   ...\n","2.      NLGN1_0      2 point(s) | + ...\n","3.    FANCD2_-2      5 point(s) | + ...\n","4.    RPUSD3_-1      4 point(s) | + ...\n","5.      MIR31_2      5 point(s) | + ...\n","6.     DCTN4_-1     -3 point(s) | + ...\n","7.     PLXDC1_1     -2 point(s) | + ...\n","8.     PCDH20_1      2 point(s) | + ...\n","9.    ZNF287_-1     -2 point(s) | + ...\n","10.        SI_-1     -2 point(s) | + ...\n","                          SCORE | =    \n","SCORE |  -11.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n","RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.6% |   1.6% |   4.4% |  11.4% |\n","SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  18.0  |\n","RISK  |  26.4% |  50.0% |  73.6% |  88.6% |  95.6% |  98.4% |  99.4% |  99.8% |  99.9% | 100.0% | 100.0% | 100.0% | 100.0% | 100.0% |\n","Training Loss:  734.7188126546931\n","Test Loss:  85.75801738769364\n","Accuracy:  0.9654663518299882 \n","AUC:  0.9654932654711335\n","Accuracy:  0.9801901743264659 \n","AUC:  0.976268951878708\n"]}],"source":["df = pd.read_csv(\"cup.CSV\")\n","primary = df['Primary']\n","df = df.iloc[:,:-1]\n","df= pd.get_dummies(df,columns = df.columns)\n","df = pd.concat([df,primary],axis=1)\n","df.replace(df.iloc[:,-1].unique()[9],1,inplace=True)\n","df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,8]],-1,inplace=True)\n","\n","df2 = pd.read_csv(\"cup2.csv\")\n","primary2 = df2['Primary']\n","df2 = df2.iloc[:,:-1]\n","df2= pd.get_dummies(df2,columns = df2.columns)\n","df2 = pd.concat([df2,primary2],axis=1)\n","df2.replace(df2.iloc[:,-1].unique()[4],1,inplace=True)\n","df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,5,6,7,8,9]],-1,inplace=True)\n","empty = pd.DataFrame(columns = h)\n","df2 = pd.concat([empty, df2], axis=0, join='outer')\n","df2 = df2[df2.columns.intersection(h)]\n","df2.fillna(0, inplace=True)\n","\n","X_train = df.iloc[:,:-1].to_numpy()\n","y_train = df['Primary'].to_numpy()\n","\n","X_test = df2.iloc[:,:-1].to_numpy()\n","y_test = df2['Primary'].to_numpy()\n","\n","from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n","\n","sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n","\n","# initialize a risk score optimizer\n","m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n","\n","# perform optimization\n","m.optimize()\n","\n","# get all top m solutions from the final diverse pool\n","arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n","\n","# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n","multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n","X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n","\n","# create a classifier\n","clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n","\n","# get the predicted label\n","y_pred = clf.predict(X = X_train)\n","\n","# get the probability of predicting y[i] with label +1\n","y_pred_prob = clf.predict_prob(X = X_train)\n","\n","# compute the logistic loss\n","logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n","logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n","# print the risk score model card\n","clf.print_model_card()\n","\n","# get accuracy and area under the ROC curve (AUC)\n","print(\"Training Loss: \",logisticLoss_train)\n","print(\"Test Loss: \",logisticLoss_test)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n","\n","acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n","print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fg_1oSznDWQE"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"fasterrisk","language":"python","name":"fasterrisk"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}