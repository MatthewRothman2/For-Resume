{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"f65b8adc-6b18-418d-a7e3-76f5a275e09a\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"current Diabetes Model Training\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     select * from \"diabetes_dataset_combined2.csv\";\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     SELECT\n#         \"Age\",\n#         \"BMI\",\n#         \"Glucose\",\n#         \"Insulin\",\n#         \"HbA1c_level\",\n#         \"Pregnancies\",\n#         hypertension,\n#         \"BloodPressure\",\n#         \"SkinThickness\",\n#         heart_disease,\n#         smoking_history,\n#         blood_glucose_level,\n#         \"DiabetesPedigreeFunction\",\n#         \"Outcome_1\"\n#     FROM diabetes_dataset_combined_4\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder get smoking history out of category. kept it as one column because there is a hiearchy to the smoking variable\nle = LabelEncoder()\ndiabetes_dataset_combined_2[\"smoking_history\"] = le.fit_transform(\n    diabetes_dataset_combined_2[\"smoking_history\"]\n)\n\n# Write the updated dataset to a new dataframe\ndiabetes_dataset_combined_2_encoded = diabetes_dataset_combined_2.copy()\n\n# Write the updated dataset to a CSV file\ndiabetes_dataset_combined_2_encoded.to_csv(\"diabetes_data_final_file.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Define the features and target variable\ny = diabetes_dataset_combined_2_encoded[\"Outcome_1\"]\nX = diabetes_dataset_combined_2_encoded.drop(columns=[\"Outcome_1\"])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train the DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(X_train, y_train)\nstart_time = time.time()\n\n# Make predictions on the test set\npredictions = dt.predict(X_test)\nend_time = time.time()\nruntime = end_time - start_time\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(y_test, predictions, average=\"binary\")\nrecall = recall_score(y_test, predictions, average=\"binary\")\n\n# Print test metrics\nprint(f\"Precision: {precision}\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Recall: {recall}\")\n# Calculate and print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint(f\"Confusion Matrix:\\n{cm}\")\ndataset_size = X_train.shape[0] + X_test.shape[0]\ntotal_nodes = dt.tree_.node_count\nleaves = dt.tree_.n_leaves\n# Print performance metrics\n# Print train metrics\n# Calculate train metrics\ntrain_predictions = dt.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(y_train, train_predictions, average=\"binary\")\ntrain_recall = recall_score(y_train, train_predictions, average=\"binary\")\ntrain_cm = confusion_matrix(y_train, train_predictions)\n\n# Print train metrics\nprint(f\"Train Accuracy: {train_accuracy}\")\nprint(f\"Train Precision: {train_precision}\")\nprint(f\"Train Recall: {train_recall}\")\nprint(f\"Train Confusion Matrix:\\n{train_cm}\")\n\n# Print test metrics\nprint(f\"Test Accuracy: {accuracy}\")\nprint(f\"Test Precision: {precision}\")\nprint(f\"Test Recall: {recall}\")\nprint(f\"Test Confusion Matrix:\\n{cm}\")\nprint(f\"Runtime: {runtime:.2f} seconds\")\nprint(f\"Total Nodes: {total_nodes}\")\nprint(f\"Total Leaves: {leaves}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Precision: 0.6379310344827587\nAccuracy: 0.7467532467532467\nRecall: 0.6727272727272727\nConfusion Matrix:\n[[78 21]\n [18 37]]\nTrain Accuracy: 1.0\nTrain Precision: 1.0\nTrain Recall: 1.0\nTrain Confusion Matrix:\n[[401   0]\n [  0 213]]\nTest Accuracy: 0.7467532467532467\nTest Precision: 0.6379310344827587\nTest Recall: 0.6727272727272727\nTest Confusion Matrix:\n[[78 21]\n [18 37]]\nRuntime: 0.00 seconds\nTotal Nodes: 205\nTotal Leaves: 103\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\nX = diabetes_dataset_combined_2_encoded.drop(columns=[\"Outcome_1\"])\ny = diabetes_dataset_combined_2_encoded[\"Outcome_1\"]\n\ndt = DecisionTreeClassifier()\nscores = cross_val_score(dt, X, y, cv=5)\n\nprint(\"Cross-validation scores:\", scores)\nprint(\"Mean cross-validation score:\", scores.mean())","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Cross-validation scores: [0.68181818 0.68181818 0.70779221 0.73856209 0.7124183 ]\nMean cross-validation score: 0.7044817927170868\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\n\nfrom sklearn.inspection import permutation_importance\n\ndt.fit(X, y)\n\n\nresult = permutation_importance(dt, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n\n# Create a DataFrame to display the feature importances\nfeature_importances = pd.DataFrame(\n    {\"Feature\": X.columns, \"Importance\": result.importances_mean}\n)\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\nfeature_importances","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"0ce08258-fbec-40a6-b70e-6bec71b8ae88/f65b8adc-6b18-418d-a7e3-76f5a275e09a/exports/b34ac29b-ef1a-43f9-bddd-8e10de3ec689"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Glucose</td>\n      <td>0.302734</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BMI</td>\n      <td>0.219792</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Age</td>\n      <td>0.160547</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>DiabetesPedigreeFunction</td>\n      <td>0.145964</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pregnancies</td>\n      <td>0.100651</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BloodPressure</td>\n      <td>0.070182</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>blood_glucose_level</td>\n      <td>0.035547</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Insulin</td>\n      <td>0.035547</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HbA1c_level</td>\n      <td>0.030469</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SkinThickness</td>\n      <td>0.005208</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>smoking_history</td>\n      <td>0.002344</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hypertension</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>heart_disease</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.01][\n    \"Feature\"\n].tolist()\nprint(important_features)\n# Define the target variable 'y' and the features 'X'\ny = diabetes_dataset_combined_2_encoded[\"Outcome_1\"]\nX = diabetes_dataset_combined_2_encoded[important_features]\n\n# Initialize the DecisionTreeClassifier\ndt = DecisionTreeClassifier()\n\n# Perform cross-validation\nscores = cross_val_score(dt, X, y, cv=k)\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", scores)\nprint(\"Mean cross-validation score:\", scores.mean())","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'Pregnancies', 'BloodPressure', 'blood_glucose_level', 'Insulin', 'HbA1c_level']\nCross-validation scores: [0.74025974 0.64935065 0.73376623 0.70588235 0.69934641]\nMean cross-validation score: 0.7057210763093116\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n#similar to the heart disease grid search. this is what model returned as best params after validation. It originally had multiple paramters.\n#I just wanted to double check the cross val scores by running it with the params selected.\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"splitter\": [\"random\"],\n    \"max_depth\": [50],\n    \"min_samples_split\": [10],\n    \"min_samples_leaf\": [4],\n    \"max_features\": [None],\n}\n\ndt1 = DecisionTreeClassifier()\ngrid_search = GridSearchCV(\n    estimator=dt, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2\n)\ngrid_search.fit(\n    diabetes_dataset_combined_2_encoded.drop(\"Outcome_1\", axis=1),\n    diabetes_dataset_combined_2_encoded[\"Outcome_1\"],\n)\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\nprint(best_params, best_score)\nbest_params, best_score","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Fitting 5 folds for each of 1 candidates, totalling 5 fits\n[CV] END criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, splitter=random; total time=   0.0s\n[CV] END criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, splitter=random; total time=   0.0s\n[CV] END criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, splitter=random; total time=   0.0s\n[CV] END criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, splitter=random; total time=   0.0s\n[CV] END criterion=gini, max_depth=50, max_features=None, min_samples_leaf=4, min_samples_split=10, splitter=random; total time=   0.0s\n{'criterion': 'gini', 'max_depth': 50, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'splitter': 'random'} 0.6979034037857568\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"},{"data":{"text/plain":"({'criterion': 'gini',\n  'max_depth': 50,\n  'max_features': None,\n  'min_samples_leaf': 4,\n  'min_samples_split': 10,\n  'splitter': 'random'},\n 0.6979034037857568)"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\n# Select important features based on threshold\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.01][\n    \"Feature\"\n]\nprint(important_features)\n\ny = diabetes_dataset_combined_2_encoded[\"Outcome_1\"]\nX = diabetes_dataset_combined_2_encoded[important_features]\n\ndt = DecisionTreeClassifier()\n#similar to the heart disease grid search. this is what model returned as best params after validation. It originally had multiple paramters.\n#I just wanted to double check the cross val scores by running it with the params selected.\n# Define the parameter grid for tuning\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"splitter\": [\"random\"],\n    \"max_depth\": [50],\n    \"min_samples_split\": [10],\n    \"min_samples_leaf\": [4],\n    \"max_features\": [None],\n}\n\n# Set up GridSearchCV with cross-validation\ngrid_search = GridSearchCV(\n    estimator=dt, param_grid=param_grid, cv=5, scoring=\"accuracy\"\n)\ngrid_search.fit(X, y)\n\n# Display best parameters and cross-validation results\nprint(\"Best parameters found:\", grid_search.best_params_)\nprint(\"Best cross-validation score:\", grid_search.best_score_)","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"2                      Glucose\n1                          BMI\n0                          Age\n12    DiabetesPedigreeFunction\n5                  Pregnancies\n7                BloodPressure\n11         blood_glucose_level\n3                      Insulin\n4                  HbA1c_level\nName: Feature, dtype: object\nBest parameters found: {'criterion': 'gini', 'max_depth': 50, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 10, 'splitter': 'random'}\nBest cross-validation score: 0.7435616670910787\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"# Import necessary libraries for confusion matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\n\n\nfrom sklearn.metrics import precision_score, accuracy_score\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n# Select important features based on threshold\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.01][\n    \"Feature\"\n]\n\n\n# Initialize the Decision Tree model\ndt = DecisionTreeClassifier(\n    criterion=\"gini\",\n    splitter=\"random\",\n    max_depth=50,\n    min_samples_split=10,\n    min_samples_leaf=4,\n    max_features=None,\n)\n\n\n# Fit the model\nbest_model = dt.fit(X_train, y_train)\n# Fit the model on the training data\nimport time\n\n# Start the timer\nstart_time = time.time()\n\n\n# Make predictions on the training set\ntrain_predictions = best_model.predict(X_train)\n\n# Calculate training metrics\ntrain_precision = precision_score(y_train, train_predictions)\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_recall = recall_score(y_train, train_predictions)\ntrain_cm = confusion_matrix(y_train, train_predictions)\n\n# Print training metrics\nprint(\"Training set precision:\", train_precision)\nprint(\"Training set accuracy:\", train_accuracy)\nprint(\"Training set recall:\", train_recall)\nprint(\"Training Confusion Matrix:\")\nprint(train_cm)\n\n# Make predictions on the test set\nend_time = time.time()\nruntime = end_time - start_time\n# Get the total number of nodes and leaves\n\ndataset_size = len(X)\n\n# Print the runtime, total leaves, nodes, and dataset size\nprint(f\"Runtime: {runtime:.2f} seconds\")\nprint(f\"Total nodes: {n_nodes}\")\nprint(f\"Total leaves: {n_leaves}\")\nprint(f\"Dataset size: {dataset_size}\")\n# Evaluate the model on the test set\n\n# Evaluate the model on the test set\n# Calculate confusion matrix\npredictions = best_model.predict(X_test)\ncm = confusion_matrix(y_test, predictions)\nprint(\"Confusion Matrix:\")\nprint(cm)  # Calculate precision and accuracy\nprecision = precision_score(y_test, predictions)\naccuracy = accuracy_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\n\nprint(\"Test set precision:\", precision)\nprint(\"Test set accuracy:\", accuracy)\nprint(\"Test set recall:\", recall)\nprint(\"Test Confusion Matrix:\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Training set precision: 0.8011049723756906\nTraining set accuracy: 0.8306188925081434\nTraining set recall: 0.6807511737089202\nTraining Confusion Matrix:\n[[365  36]\n [ 68 145]]\nRuntime: 0.01 seconds\nTotal nodes: 203\nTotal leaves: 102\nDataset size: 768\nConfusion Matrix:\n[[79 20]\n [19 36]]\nTest set precision: 0.6428571428571429\nTest set accuracy: 0.7467532467532467\nTest set recall: 0.6545454545454545\nTest Confusion Matrix:\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\nimport time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Define the features and target variable\ny = diabetes_dataset_combined_2[\"Outcome_1\"]\nX = diabetes_dataset_combined_2.drop(columns=[\"Outcome_1\"])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train the RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\n# Make predictions on the training set\ntrain_predictions = rf.predict(X_train)\nstart_time = time.time()\n\n\npredictions = rf.predict(X_test)\nend_time = time.time()\nruntime = end_time - start_time\n# Calculate training accuracy, precision, and recall\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(\n    y_train, train_predictions, average=\"binary\", zero_division=1\n)\ntrain_recall = recall_score(\n    y_train, train_predictions, average=\"binary\", zero_division=1\n)\n\n# Calculate training confusion matrix\ntrain_cm = confusion_matrix(y_train, train_predictions)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(y_test, predictions, average=\"binary\", zero_division=1)\nrecall = recall_score(y_test, predictions, average=\"binary\", zero_division=1)\ndataset_size = len(X)\ntotal_nodes = sum(estimator.tree_.node_count for estimator in rf.estimators_)\nleaves = sum(estimator.tree_.n_leaves for estimator in rf.estimators_)\n\n# Print precision and recall\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\n# Calculate and print the confusion matrix\ncm = confusion_matrix(y_test, predictions)\nprint(f\"Confusion Matrix:\\n{cm}\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Runtime: {runtime} seconds\")\nprint(f\"Total nodes: {total_nodes}\")\nprint(f\"Total leaves: {leaves}\")\n# Print training metrics\nprint(f\"Training Accuracy: {train_accuracy}\")\nprint(f\"Training Precision: {train_precision}\")\nprint(f\"Training Recall: {train_recall}\")\nprint(f\"Training Confusion Matrix:\\n{train_cm}\")\nprint(f\"Dataset size: {dataset_size}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Precision: 0.6666666666666666\nRecall: 0.6545454545454545\nConfusion Matrix:\n[[81 18]\n [19 36]]\nAccuracy: 0.7597402597402597\nRuntime: 0.01489567756652832 seconds\nTotal nodes: 20364\nTotal leaves: 10232\nTraining Accuracy: 1.0\nTraining Precision: 1.0\nTraining Recall: 1.0\nTraining Confusion Matrix:\n[[401   0]\n [  0 213]]\nDataset size: 768\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Use the 'diabetes_dataset_combined_2' dataset and 'heart_disease' as the target variable\nX = diabetes_dataset_combined_2.drop(columns=[\"Outcome_1\"])\ny = diabetes_dataset_combined_2[\"Outcome_1\"]\n\n# Create a random forest classifier\nrf = RandomForestClassifier()\n\n# Fit the model on the training data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Fit the model on the training data\nrf.fit(X_train, y_train)\n\n# Predict on the test data\npredictions = rf.predict(X_test)\n\n# Evaluate the model\ntest_score = rf.score(X_test, y_test)\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\n# Get feature importances\nfeature_importances = pd.DataFrame(\n    {\"Feature\": X.columns, \"Importance\": rf.feature_importances_}\n)\n\n# Sort the feature importances in descending order\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\n# Display the feature importances\nprint(feature_importances)","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"                     Feature  Importance\n2                    Glucose    0.235802\n1                        BMI    0.139330\n0                        Age    0.124804\n12  DiabetesPedigreeFunction    0.097652\n7              BloodPressure    0.070883\n5                Pregnancies    0.067739\n3                    Insulin    0.059501\n11       blood_glucose_level    0.057952\n8              SkinThickness    0.057472\n4                HbA1c_level    0.049703\n10           smoking_history    0.027819\n6               hypertension    0.007952\n9              heart_disease    0.003392\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\n\n# Import necessary libraries\nfrom sklearn.model_selection import GridSearchCV\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Define the grid search model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"max_depth\": [6],\n    \"max_features\": [\"auto\"],\n    \"n_estimators\": [200],\n}\ngrid_search = GridSearchCV(\n    estimator=rf, param_grid=param_grid, cv=10, n_jobs=-1, scoring=\"accuracy\"\n)\n\n\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.01][\n    \"Feature\"\n].tolist()\n\n# Prepare the data\nX = diabetes_dataset_combined_2.drop(columns=[\"Outcome_1\"])\ny = diabetes_dataset_combined_2[\"Outcome_1\"]  # Target variable 'Outcome_1'\n\n# Train the model\nscores = cross_val_score(rf, X[important_features], y, cv=10)\n\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Mean cross-validation score: {scores.mean()}\")\n\n# Make predictions\ngrid_search.fit(X[important_features], y)\n\n\nrf.fit(X_train[important_features], y_train)  # Train the model on the training set\n\ny_pred = rf.predict(X_test[important_features])\n\n# Print feature importance columns\nprint(f\"Features used in the prediction: {important_features}\")\nprint(f\"important_features = {important_features}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Cross-validation scores: [0.7012987  0.79220779 0.80519481 0.67532468 0.7012987  0.77922078\n 0.81818182 0.81818182 0.75       0.80263158]\nMean cross-validation score: 0.7643540669856459\nFeatures used in the prediction: ['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'BloodPressure', 'Pregnancies', 'Insulin', 'blood_glucose_level', 'SkinThickness', 'HbA1c_level', 'smoking_history']\nimportant_features = ['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'BloodPressure', 'Pregnancies', 'Insulin', 'blood_glucose_level', 'SkinThickness', 'HbA1c_level', 'smoking_history']\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport time\n\n# Filter features based on importance\nselected_features = feature_importances[feature_importances[\"Importance\"] > 0.01][\n    \"Feature\"\n].tolist()\nX_train, X_test, y_train, y_test = train_test_split(\n    X[selected_features], y, test_size=0.2, random_state=42\n)\nprint(\"Selected Features:\", X_train.columns)\n\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"max_depth\": [6],\n    \"max_features\": [\"auto\"],\n    \"n_estimators\": [200],\n}\n\nstart_time = time.time()\ngrid_search.fit(X_train, y_train)\nend_time = time.time()\nruntime = end_time - start_time\nbest_params = grid_search.best_params_\nbest_estimator = grid_search.best_estimator_\nypred = best_estimator.predict(X_test)\ntest_accuracy = accuracy_score(y_test, ypred)\ncm = confusion_matrix(y_test, ypred)\nprint(f\"Best Parameters: {best_params}\")\nprint(f\"Accuracy: {accuracy}\")\ntest_precision = precision_score(y_test, ypred)\ntest_recall = recall_score(y_test, ypred)\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")\nprint(f\"Test Confusion Matrix:\\n{cm}\")\ndataset_size = len(X_train) + len(X_test)\ntrain_predictions = best_estimator.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(y_train, train_predictions)\ntrain_recall = recall_score(y_train, train_predictions)\ntrain_cm = confusion_matrix(y_train, train_predictions)\nprint(f\"Train Accuracy: {train_accuracy}\")\nprint(f\"Train Precision: {train_precision}\")\nprint(f\"Train Recall: {train_recall}\")\nprint(f\"Train Confusion Matrix:\\n{train_cm}\")\ntotal_nodes = best_estimator.estimators_[0].tree_.node_count\nleaves = best_estimator.estimators_[0].tree_.n_leaves\nprint(f\"Runtime: {runtime} seconds\")\nprint(f\"Total Nodes: {total_nodes}\")\nprint(f\"Total Leaves: {leaves}\")\nprint(f\"Dataset Size: {dataset_size}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Selected Features: Index(['Glucose', 'BMI', 'Age', 'DiabetesPedigreeFunction', 'BloodPressure',\n       'Pregnancies', 'Insulin', 'blood_glucose_level', 'SkinThickness',\n       'HbA1c_level', 'smoking_history'],\n      dtype='object')\nBest Parameters: {'criterion': 'gini', 'max_depth': 6, 'max_features': 'auto', 'n_estimators': 200}\nAccuracy: 0.7597402597402597\nTest Accuracy: 0.7467532467532467\nTest Precision: 0.6481481481481481\nTest Recall: 0.6363636363636364\nTest Confusion Matrix:\n[[80 19]\n [20 35]]\nTrain Accuracy: 0.9218241042345277\nTrain Precision: 0.9459459459459459\nTrain Recall: 0.8215962441314554\nTrain Confusion Matrix:\n[[391  10]\n [ 38 175]]\nRuntime: 4.40130877494812 seconds\nTotal Nodes: 75\nTotal Leaves: 38\nDataset Size: 768\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Import confusion_matrix (already imported in the environment)\n# from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\n\ndrop_columns = [\"Outcome_1\"]\nX = diabetes_dataset_combined_2.drop(columns=drop_columns)\ny = diabetes_dataset_combined_2[\"Outcome_1\"]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Initialize and train the logistic regression model\nlogreg = LogisticRegression(max_iter=1000)\nlogreg.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = logreg.predict(X_test)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n# Print accuracy, precision, and recall\n# Print train metrics\ntrain_predictions = logreg.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(y_train, train_predictions)\ntrain_recall = recall_score(y_train, train_predictions)\nprint(f\"Train Accuracy: {train_accuracy:.2f}\")\nprint(f\"Train Precision: {train_precision:.2f}\")\nprint(f\"Train Recall: {train_recall:.2f}\")\n\n# Print test metrics\nprint(f\"Test Accuracy: {accuracy:.2f}\")\nprint(f\"Test Precision: {precision:.2f}\")\nprint(f\"Test Recall: {recall:.2f}\")\n# Display the confusion matrix\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n# Print test metric\nprint(\n    f\"Test Metric - Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}\"\n)","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Train Accuracy: 0.77\nTrain Precision: 0.72\nTrain Recall: 0.56\nTest Accuracy: 0.76\nTest Precision: 0.66\nTest Recall: 0.67\nConfusion Matrix:\n[[80 19]\n [18 37]]\nTest Metric - Accuracy: 0.76, Precision: 0.66, Recall: 0.67\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n# Fit the logistic regression model\nlogreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train, y_train)\n\n# Calculate permutation importance\nresult = permutation_importance(\n    logreg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n)\n\n# Create a DataFrame for permutation importance\nfeature_importances = pd.DataFrame(\n    {\"Feature\": X_test.columns, \"Importance\": result.importances_mean}\n)\n\n# Sort the DataFrame by importance\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\nfeature_importances\n\n# Print all the features considered\nprint(\"Features considered:\", X_test.columns.tolist())","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Features considered: ['Age', 'BMI', 'Glucose', 'Insulin', 'HbA1c_level', 'Pregnancies', 'hypertension', 'BloodPressure', 'SkinThickness', 'heart_disease', 'smoking_history', 'blood_glucose_level', 'DiabetesPedigreeFunction']\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport numpy as np\n\n# Fit the logistic regression model\nlogreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train, y_train)\n\n# Get feature importance\nfeature_importance = np.abs(logreg.coef_[0])\n\n# Create a DataFrame for feature importance\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": X_train.columns, \"Importance\": feature_importance}\n)\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfeature_importance_df","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"0ce08258-fbec-40a6-b70e-6bec71b8ae88/f65b8adc-6b18-418d-a7e3-76f5a275e09a/exports/b5675637-ae50-458b-b29f-583853c30b1e"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>DiabetesPedigreeFunction</td>\n      <td>0.642799</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hypertension</td>\n      <td>0.234787</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BMI</td>\n      <td>0.102141</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>heart_disease</td>\n      <td>0.079439</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pregnancies</td>\n      <td>0.062275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HbA1c_level</td>\n      <td>0.040482</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Age</td>\n      <td>0.037823</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Glucose</td>\n      <td>0.034204</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>smoking_history</td>\n      <td>0.025301</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BloodPressure</td>\n      <td>0.013569</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SkinThickness</td>\n      <td>0.003193</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>blood_glucose_level</td>\n      <td>0.002632</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Insulin</td>\n      <td>0.001763</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.inspection import permutation_importance\n\n# Fit the logistic regression model\nlogreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train, y_train)\n\n# Calculate permutation importance\nresult = permutation_importance(logreg, X_test, y_test, n_repeats=10, random_state=42)\n\n# Create a DataFrame for permutation importance\nfeature_importances = pd.DataFrame(\n    {\"Feature\": X_test.columns, \"Importance\": result.importances_mean}\n)\n\n# Sort the DataFrame by importance\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\nfeature_importances","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"0ce08258-fbec-40a6-b70e-6bec71b8ae88/f65b8adc-6b18-418d-a7e3-76f5a275e09a/exports/6389cd67-c6e5-4392-b592-268d2a36457b"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>Glucose</td>\n      <td>0.138961</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BMI</td>\n      <td>0.030519</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>DiabetesPedigreeFunction</td>\n      <td>0.029870</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>blood_glucose_level</td>\n      <td>0.018182</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Insulin</td>\n      <td>0.017532</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BloodPressure</td>\n      <td>0.008442</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>smoking_history</td>\n      <td>0.005844</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SkinThickness</td>\n      <td>0.003247</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HbA1c_level</td>\n      <td>0.002597</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>heart_disease</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hypertension</td>\n      <td>-0.002597</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pregnancies</td>\n      <td>-0.002597</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Age</td>\n      <td>-0.011039</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Filter features with importance above 0.01\nimportant_features = feature_importances[feature_importance_df[\"Importance\"] > 0.01][\n    \"Feature\"\n].tolist()\nX_filtered = X[important_features]\n\n# Initialize Logistic Regression model\nlogreg = LogisticRegression(max_iter=1000)\n\n# Perform cross-validation\ncv_scores = cross_val_score(logreg, X_filtered, y, cv=5)\n\n# Print cross-validation scores\ncv_scores","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"array([0.75324675, 0.74025974, 0.75974026, 0.79738562, 0.75163399])"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\n\n# Fit the logistic regression model\nlogreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train, y_train)\n\n# Get feature importance\nfeature_importance = np.abs(logreg.coef_[0])\n\n# Create a DataFrame for feature importance\nfeature_importance_df = pd.DataFrame(\n    {\"Feature\": X_train.columns, \"Importance\": feature_importance}\n)\n\nfeature_importance_df = feature_importance_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\nfeature_importance_df","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"0ce08258-fbec-40a6-b70e-6bec71b8ae88/f65b8adc-6b18-418d-a7e3-76f5a275e09a/exports/ceedbdad-4c75-4050-ad9b-bd52e166f07d"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12</th>\n      <td>DiabetesPedigreeFunction</td>\n      <td>0.642799</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>hypertension</td>\n      <td>0.234787</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BMI</td>\n      <td>0.102141</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>heart_disease</td>\n      <td>0.079439</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Pregnancies</td>\n      <td>0.062275</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HbA1c_level</td>\n      <td>0.040482</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>Age</td>\n      <td>0.037823</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Glucose</td>\n      <td>0.034204</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>smoking_history</td>\n      <td>0.025301</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>BloodPressure</td>\n      <td>0.013569</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>SkinThickness</td>\n      <td>0.003193</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>blood_glucose_level</td>\n      <td>0.002632</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Insulin</td>\n      <td>0.001763</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Filter features with different importance thresholds and return the accuracy, precision, and recall for each threshold\nimportant_features = feature_importance_df[feature_importance_df[\"Importance\"] > 0.01][\n    \"Feature\"\n].tolist()\nX_filtered = X[important_features]\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid = {\n    \"C\": [0.01, 0.1, 1, 10, 100],\n    \"max_iter\": [100, 200, 300],\n    \"penalty\": [\"l1\", \"l2\"],\n    \"solver\": [\"liblinear\", \"saga\"],\n}\n\n# Initialize the logistic regression model\nlogreg = LogisticRegression()\n\n# Initialize GridSearchCV\ngrid_search = GridSearchCV(\n    estimator=logreg, param_grid=param_grid, cv=5, scoring=\"accuracy\"\n)\n\n# Fit the model\ngrid_search.fit(X_filtered, y)\n\n# Get the best parameters and best score\nbest_params = grid_search.best_params_\nbest_score = grid_search.best_score_\n\n# Perform cross-validation\ncv_scores = cross_val_score(grid_search.best_estimator_, X_filtered, y, cv=5)\nprecision_scores = cross_val_score(\n    grid_search.best_estimator_, X_filtered, y, cv=5, scoring=\"precision\"\n)\nrecall_scores = cross_val_score(\n    grid_search.best_estimator_, X_filtered, y, cv=5, scoring=\"recall\"\n)\n\n# Store results\nresults = {\n    0.01: {\n        \"best_params\": best_params,\n        \"best_score\": best_score,\n        \"cv_scores\": cv_scores,\n        \"precision_scores\": precision_scores,\n        \"recall_scores\": recall_scores,\n    }\n}\n\n# Print results\nfor threshold, result in results.items():\n    print(f\"Threshold: {threshold}\")\n    print(\"Best parameters:\", result[\"best_params\"])\n    print(\"Best score:\", result[\"best_score\"])\n    print(\"Cross-validation scores:\", result[\"cv_scores\"])\n    print(\"Cross-validation precision scores:\", result[\"precision_scores\"])\n    print(\"Cross-validation recall scores:\", result[\"recall_scores\"])","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Threshold: 0.01\nBest parameters: {'C': 1, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\nBest score: 0.7682794329853154\nCross-validation scores: [0.75324675 0.74675325 0.76623377 0.81045752 0.76470588]\nCross-validation precision scores: [0.68181818 0.64705882 0.73684211 0.78571429 0.74285714]\nCross-validation recall scores: [0.55555556 0.61111111 0.51851852 0.62264151 0.49056604]\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nthreshold = 0.01\nimportant_features = []\n\n# Define the parameters for the logistic regression model\nparams = {\"C\": 1, \"max_iter\": 100, \"penalty\": \"l1\", \"solver\": \"liblinear\"}\n\n# Initialize the logistic regression model with the defined parameters\nlogreg = LogisticRegression(**params)\n\n# Dictionary to store results\nresults = {}\n\nimportant_features = feature_importance_df[\n    feature_importance_df[\"Importance\"] > threshold\n][\"Feature\"].tolist()\nprint(f\"Features used for threshold {threshold}: {important_features}\")\nX_filtered = X[important_features]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_filtered, y, test_size=0.20, random_state=42\n)\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# Make predictions on the training set\ntrain_predictions = logreg.predict(X_train)\n\n# Calculate training accuracy, precision, and recall\ntrain_accuracy = accuracy_score(y_train, train_predictions)\ntrain_precision = precision_score(y_train, train_predictions)\ntrain_recall = recall_score(y_train, train_predictions)\n\n# Calculate training confusion matrix\ntrain_cm = confusion_matrix(y_train, train_predictions)\nprint(\"Training Confusion Matrix:\")\nprint(train_cm)\n\n# Get the best parameters and best score\nbest_params = params\nbest_score = logreg.score(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = logreg.predict(X_test)\n\n# Calculate accuracy, precision, and recall\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\n\n# Calculate confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Confusion Matrix:\")\nprint(cm)\n\n# Store results\nresults[threshold] = {\n    \"best_params\": best_params,\n    \"best_score\": best_score,\n    \"test_accuracy\": accuracy,\n    \"test_precision\": precision,\n    \"test_recall\": recall,\n    \"train_accuracy\": train_accuracy,\n    \"train_precision\": train_precision,\n    \"train_recall\": train_recall,\n}\n\n# Print results\nfor threshold, result in results.items():\n    print(f\"Threshold: {threshold}\")\n    print(\"Best parameters:\", result[\"best_params\"])\n    print(\"Best score:\", result[\"best_score\"])\n    print(\"Test accuracy:\", result[\"test_accuracy\"])\n    print(\"Test precision:\", result[\"test_precision\"])\n    print(\"Test recall:\", result[\"test_recall\"])\n    print(\"Train accuracy:\", result[\"train_accuracy\"])\n    print(\"Train precision:\", result[\"train_precision\"])\n    print(\"Train recall:\", result[\"train_recall\"])","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Features used for threshold 0.01: ['DiabetesPedigreeFunction', 'hypertension', 'BMI', 'heart_disease', 'Pregnancies', 'HbA1c_level', 'Age', 'Glucose', 'smoking_history', 'BloodPressure']\nTraining Confusion Matrix:\n[[350  51]\n [ 93 120]]\nConfusion Matrix:\n[[80 19]\n [19 36]]\nThreshold: 0.01\nBest parameters: {'C': 1, 'max_iter': 100, 'penalty': 'l1', 'solver': 'liblinear'}\nBest score: 0.7654723127035831\nTest accuracy: 0.7532467532467533\nTest precision: 0.6545454545454545\nTest recall: 0.6545454545454545\nTrain accuracy: 0.7654723127035831\nTrain precision: 0.7017543859649122\nTrain recall: 0.5633802816901409\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\nX = diabetes_dataset_combined_2_encoded.drop(columns=[\"Outcome_1\"])\ny = diabetes_dataset_combined_2_encoded[\"Outcome_1\"]\n\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define the Perceptron model\nperceptron = Perceptron()\n\n# Fit the model on the training data\nperceptron.fit(X_train, y_train)\n\n# Make predictions on the test data\ny_pred = perceptron.predict(X_test)\n\n# Evaluate the model\ntest_precision = precision_score(y_test, y_pred, average=\"binary\")\ntest_recall = recall_score(y_test, y_pred, average=\"binary\")\n\n# Print the evaluation metrics\nprint(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Test Accuracy: 0.38961038961038963\nTest Precision: 0.3691275167785235\nTest Recall: 1.0\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Matt Rothman","project_id":"f65b8adc-6b18-418d-a7e3-76f5a275e09a","version":"draft","exported_date":"Sun Dec 15 2024 21:33:46 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}