{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"5201d03d-79a9-4f43-82b3-8a89c43ad1f3\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"Current model training Heart Disease full dataset\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#4C78A8\\\",\\\"#F58518\\\",\\\"#E45756\\\",\\\"#72B7B2\\\",\\\"#54A24B\\\",\\\"#EECA3B\\\",\\\"#B279A2\\\",\\\"#FF9DA6\\\",\\\"#9D755D\\\",\\\"#BAB0AC\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     select * from \"heart_diseases.csv\";\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     SELECT\n#         ca,\n#         age,\n#         fbs,\n#         chol,\n#         \"CK-MB\",\n#         exang,\n#         thalch,\n#         oldpeak,\n#         \"Troponin\",\n#         \"sex_Male\",\n#         trestbps,\n#         \"Heart rate\",\n#         \"sex_Female\",\n#         slope_flat,\n#         \"Blood sugar\",\n#         thal_normal,\n#         restecg_normal,\n#         cp_asymptomatic,\n#         \"dataset_Hungary\",\n#         slope_upsloping,\n#         \"cp_typical angina\",\n#         \"dataset_Cleveland\",\n#         slope_downsloping,\n#         \"thal_fixed defect\",\n#         \"cp_atypical angina\",\n#         \"dataset_VA Long Beach\",\n#         \"restecg_lv hypertrophy\",\n#         \"thal_reversable defect\",\n#         \"Systolic blood pressure\",\n#         \"Diastolic blood pressure\",\n#         \"restecg_st-t abnormality\",\n#         num\n#     FROM heart_diseases_2;\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import confusion_matrix\n# import time to count runtime of the model\n# from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\n\n\n\n\nstart_time = time.time()\nX = heart_diseases.drop(columns=[\"num\"])\ny = heart_diseases[\"num\"]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ntrain_score = dt.score(X_train, y_train)\ntest_score = dt.score(X_test, y_test)\nruntime = time.time() - start_time\n\n# Calculate precision and recall\nprecision = precision_score(y_test, dt.predict(X_test), average=\"weighted\")\nrecall = recall_score(y_test, dt.predict(X_test), average=\"weighted\")\n\n# Print train accuracy, test accuracy, precision, recall, and confusion matrix\nprint(f\"Train accuracy: {train_score}\")\nprint(f\"Test accuracy: {test_score}\")\nprint(\n    f\"Train Precision: {precision_score(y_train, dt.predict(X_train), average='weighted')}\"\n)\nprint(f\"Train Recall: {recall_score(y_train, dt.predict(X_train), average='weighted')}\")\nprint(f\"Test Precision: {precision}\")\nprint(f\"Test Recall: {recall}\")\nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_test, dt.predict(X_test))}\")\nprint(f\"Runtime: {runtime:.2f} seconds\")\nprint(f\"Dataset size: {len(X)}\")\nprint(f\"Confusion Matrix (Train):\\n{confusion_matrix(y_train, dt.predict(X_train))}\")\nprint(f\"Confusion Matrix (Test):\\n{confusion_matrix(y_test, dt.predict(X_test))}\")\nprint(f\"Number of nodes: {dt.tree_.node_count}\")\nprint(f\"Number of leaves: {dt.get_n_leaves()}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Train accuracy: 1.0\nTest accuracy: 0.7166666666666667\nTrain Precision: 1.0\nTrain Recall: 1.0\nTest Precision: 0.7235863095238095\nTest Recall: 0.7166666666666667\nConfusion Matrix:\n[[25 10]\n [ 7 18]]\nRuntime: 0.01 seconds\nDataset size: 299\nConfusion Matrix (Train):\n[[125   0]\n [  0 114]]\nConfusion Matrix (Test):\n[[25 10]\n [ 7 18]]\nNumber of nodes: 77\nNumber of leaves: 39\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"# Fit Decision Tree\n\nfrom sklearn.inspection import permutation_importance\n\ndt.fit(X, y)\n\nresult = permutation_importance(dt, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n\n# Create a DataFrame to display the feature importances\nfeature_importances = pd.DataFrame(\n    {\"Feature\": X.columns, \"Importance\": result.importances_mean}\n)\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\nfeature_importances","metadata":{},"execution_count":null,"outputs":[{"data":{"application/vnd.hex.export+parquet":{"success":true,"exportKey":"0ce08258-fbec-40a6-b70e-6bec71b8ae88/5201d03d-79a9-4f43-82b3-8a89c43ad1f3/exports/e93f1d71-1a2e-4fd5-8b9e-71a56ddff251"},"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>thal_normal</td>\n      <td>0.209030</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>ca</td>\n      <td>0.172575</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>cp_asymptomatic</td>\n      <td>0.152508</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>age</td>\n      <td>0.094649</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Systolic blood pressure</td>\n      <td>0.077258</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>oldpeak</td>\n      <td>0.071572</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>trestbps</td>\n      <td>0.056856</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>chol</td>\n      <td>0.055853</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Blood sugar</td>\n      <td>0.043813</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Diastolic blood pressure</td>\n      <td>0.028763</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>thalch</td>\n      <td>0.025753</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Heart rate</td>\n      <td>0.021070</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sex_Male</td>\n      <td>0.016388</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Troponin</td>\n      <td>0.012709</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CK-MB</td>\n      <td>0.009030</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>restecg_lv hypertrophy</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>cp_atypical angina</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>dataset_VA Long Beach</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fbs</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>thal_reversable defect</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>slope_downsloping</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>thal_fixed defect</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>restecg_normal</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>dataset_Cleveland</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>cp_typical angina</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>slope_upsloping</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>dataset_Hungary</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>slope_flat</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sex_Female</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>exang</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>restecg_st-t abnormality</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.05][\n    \"Feature\"\n].tolist()\nprint(important_features)\n# Define the target variable 'y' and the features 'X'\ny = heart_diseases[\"num\"]\nX = heart_diseases[important_features]\n\n# Initialize the DecisionTreeClassifier\ndt = DecisionTreeClassifier()\n\n# Perform cross-validation\nscores = cross_val_score(dt, X, y, cv=5)\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", scores)\nprint(\"Mean cross-validation score:\", np.mean(scores))","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"['thal_normal', 'ca', 'cp_asymptomatic', 'age', 'Systolic blood pressure', 'oldpeak', 'trestbps', 'chol']\nCross-validation scores: [0.7        0.8        0.71666667 0.75       0.74576271]\nMean cross-validation score: 0.7424858757062147\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np\n\n# Select important features based on threshold from the feature importance\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.05][\"Feature\"]\nprint(important_features)\n\n# Define the target variable 'y' and the features 'X' using the important features\ny = heart_diseases[\"num\"]\nX = heart_diseases[important_features]\n\n\ndt = DecisionTreeClassifier()\n\n# Define a parameter grid for tuning\nparam_grid = {\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"splitter\": [\"best\", \"random\"],\n    \"max_depth\": [None, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n    \"max_features\": [None, \"sqrt\", \"log2\"],\n}\n\n# Set up GridSearchCV with cross-validation\ngrid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring=\"accuracy\")\ngrid_search.fit(X, y)\n\n# Display the best parameters and cross-validation results\nprint(\"Best parameters found:\", grid_search.best_params_)\nprint(\"Best cross-validation score:\", grid_search.best_score_)\n","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"15                thal_normal\n0                          ca\n17            cp_asymptomatic\n1                         age\n28    Systolic blood pressure\n7                     oldpeak\n10                   trestbps\n3                        chol\nName: Feature, dtype: object\nBest parameters found: {'criterion': 'entropy', 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 10, 'splitter': 'random'}\nBest cross-validation score: 0.832598870056497\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import numpy as np\nimport numpy as np\n\n# Calculate and print precision, accuracy, and recall for test and train\n# Compare train metrics to test metrics\nimport time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, accuracy_score, recall_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.05][\n    \"Feature\"\n]\nprint(\"Selected important features:\", list(important_features))\nprint(\"Selected important features:\", important_features)\n# Define the target variable 'y' and the features 'X'\ny = heart_diseases[\"num\"]\nX = heart_diseases[important_features]\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42\n)\n# create empty list to then append to. Then take the mean from these lists to get the average score\n\ntrain_precision_scores = []\ntrain_recall_scores = []\ntrain_accuracy_scores = []\ntest_precision_scores = []\ntest_recall_scores = []\ntest_accuracy_scores = []\n#run the model 100 times and get \nfor _ in range(100):\n    start_time = time.time()\n    # Initialize the DecisionTreeClassifier with the specified configuration\n    dt = DecisionTreeClassifier(\n        criterion=\"entropy\",\n        splitter=\"random\",\n        max_depth=20,\n        min_samples_split=10,\n        min_samples_leaf=4,\n        max_features=None,\n    )\n    # Train the model\n    dt.fit(X_train, y_train)\n    # Make predictions\n    y_pred = dt.predict(X_test)\n    # Make predictions on the training set\n    y_train_pred = dt.predict(X_train)\n    # Calculate and print precision, accuracy, and recall\n    train_precision = precision_score(y_train, y_train_pred)\n    train_recall = recall_score(y_train, y_train_pred)\n    train_accuracy = accuracy_score(y_train, y_train_pred)\n\n    # Calculate precision, recall, and accuracy for the test set\n    test_precision = precision_score(y_test, y_pred)\n    test_recall = recall_score(y_test, y_pred)\n    test_accuracy = accuracy_score(y_test, y_pred)\n\n    # Accumulate scores for averaging\n    train_precision_scores.append(train_precision)\n    train_recall_scores.append(train_recall)\n    train_accuracy_scores.append(train_accuracy)\n    test_precision_scores.append(test_precision)\n    test_recall_scores.append(test_recall)\n    test_accuracy_scores.append(test_accuracy)\n\n#take the average of the list and print those score\nprint(f\"Average Train Precision: {np.mean(train_precision_scores):.2f}\")\nprint(f\"Average Train Recall: {np.mean(train_recall_scores):.2f}\")\nprint(f\"Average Train Accuracy: {np.mean(train_accuracy_scores):.2f}\")\nprint(f\"Average Test Precision: {np.mean(test_precision_scores):.2f}\")\nprint(f\"Average Test Recall: {np.mean(test_recall_scores):.2f}\")\nprint(f\"Average Test Accuracy: {np.mean(test_accuracy_scores):.2f}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Selected important features: ['thal_normal', 'ca', 'cp_asymptomatic', 'age', 'Systolic blood pressure', 'oldpeak', 'trestbps', 'chol']\nSelected important features: 15                thal_normal\n0                          ca\n17            cp_asymptomatic\n1                         age\n28    Systolic blood pressure\n7                     oldpeak\n10                   trestbps\n3                        chol\nName: Feature, dtype: object\nAverage Train Precision: 0.87\nAverage Train Recall: 0.81\nAverage Train Accuracy: 0.85\nAverage Test Precision: 0.80\nAverage Test Recall: 0.75\nAverage Test Accuracy: 0.81\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reimport libraries just in case\nimport time\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Define the features and target variable\ny = heart_diseases[\"num\"]\nX = heart_diseases.drop(columns=[\"num\"])\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Create and train the RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nstart_time = time.time()\nrf.fit(X_train, y_train)\nruntime = time.time() - start_time\n\n# Make predictions on the test set\npredictions = rf.predict(X_test)\n\n# Calculate the accuracy\naccuracy = accuracy_score(y_test, predictions)\n\n# Calculate precision and recall\nprecision = precision_score(y_test, predictions, average=\"binary\")\nrecall = recall_score(y_test, predictions, average=\"binary\")\n\n# Calculate train and test scores\ntrain_score = rf.score(X_train, y_train)\ntest_score = rf.score(X_test, y_test)\n\n# Calculate the number of nodes and leaves in the forest\nn_nodes = rf.n_estimators\nn_nodes = sum(estimator.tree_.node_count for estimator in rf.estimators_)\ndataset_size = X_train.shape[0] + X_test.shape[0]\n\n# Print performance metrics\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\ncm = confusion_matrix(y_test, predictions)\nprint(f\"Confusion Matrix:\\n{cm}\")\nprint(f\"Dataset Size: {dataset_size}\")\nprint(f\"Number of Nodes: {n_nodes}\")\nprint(f\"Runtime: {runtime:.2f} seconds\")\n\n# Compare train metrics to test metrics\nprint(f\"Train Accuracy: {train_score}\")\nprint(f\"Test Accuracy: {test_score}\")\nprint(\n    f\"Train Precision: {precision_score(y_train, rf.predict(X_train), average='binary')}\"\n)\nprint(f\"Test Precision: {precision}\")\nprint(f\"Train Recall: {recall_score(y_train, rf.predict(X_train), average='binary')}\")\nprint(f\"Test Recall: {recall}\")\n\n\n# Print total number of nodes and leaves\nprint(f\"Total Number of Nodes: {n_nodes}\")\nprint(f\"Total Number of Leaves: {n_leaves}\")\nprint(f\"Number of Root Nodes: {n_root_nodes}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Accuracy: 0.9\nPrecision: 0.9130434782608695\nRecall: 0.84\nConfusion Matrix:\n[[33  2]\n [ 4 21]]\nDataset Size: 299\nNumber of Nodes: 7956\nRuntime: 0.18 seconds\nTrain Accuracy: 1.0\nTest Accuracy: 0.9\nTrain Precision: 1.0\nTest Precision: 0.9130434782608695\nTrain Recall: 1.0\nTest Recall: 0.84\nTotal Number of Nodes: 7956\nTotal Number of Leaves: 6633\nNumber of Root Nodes: 200\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.inspection import permutation_importance\n\n#define varibales and target\nX = heart_diseases.drop(columns=[\"num\"])\ny = heart_diseases[\"num\"]\n\n# Create a random forest classifier\nrf = RandomForestClassifier()\n\n# Fit the model on the training data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Fit the model on the training data\nrf.fit(X_train, y_train)\n\n# Predict on the test data\npredictions = rf.predict(X_test)\n\n# Evaluate the model\ntest_score = rf.score(X_test, y_test)\nprecision = precision_score(y_test, predictions)\nrecall = recall_score(y_test, predictions)\n\n# Get permutation importances\nresult = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42)\nfeature_importances = pd.DataFrame(\n    {\n        \"Feature\": X.columns,\n        \"Importance\": result.importances_mean / result.importances_mean.sum(),\n    }\n)\n\n# Sort the feature importances in descending order\nfeature_importances = feature_importances.sort_values(by=\"Importance\", ascending=False)\n\n# Display the feature importances\nprint(feature_importances.reset_index(drop=True))","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"                     Feature  Importance\n0            cp_asymptomatic    0.209877\n1                         ca    0.172840\n2                    oldpeak    0.135802\n3    Systolic blood pressure    0.123457\n4                thal_normal    0.111111\n5     thal_reversable defect    0.074074\n6                     thalch    0.074074\n7                        age    0.061728\n8                 slope_flat    0.049383\n9   Diastolic blood pressure    0.049383\n10           slope_upsloping    0.049383\n11                  trestbps    0.024691\n12               Blood sugar    0.024691\n13                     CK-MB    0.024691\n14                  Troponin    0.012346\n15     dataset_VA Long Beach    0.000000\n16        cp_atypical angina    0.000000\n17           dataset_Hungary    0.000000\n18    restecg_lv hypertrophy    0.000000\n19         slope_downsloping    0.000000\n20                       fbs    0.000000\n21         thal_fixed defect    0.000000\n22  restecg_st-t abnormality    0.000000\n23         dataset_Cleveland    0.000000\n24         cp_typical angina    0.000000\n25            restecg_normal    0.000000\n26                sex_Female    0.000000\n27                  sex_Male    0.000000\n28                      chol   -0.037037\n29                Heart rate   -0.061728\n30                     exang   -0.098765\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import necessary libraries\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\n\n# Import necessary libraries\n\n# Define the parameter grid for GridSearchCV. As a note to the professors. There is only one hyper param because I was checking the validation accuracy. \n#This is what the model chose as the best hyper params\n#after running the grid search on a few different hyper params\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"max_depth\": [6],\n    \"max_features\": [\"auto\"],\n    \"n_estimators\": [200],\n}\n# Define the grid search model\ngrid_search = GridSearchCV(\n    estimator=rf, param_grid=param_grid, cv=10, n_jobs=-1, scoring=\"accuracy\"\n)\nimportant_features = feature_importances[feature_importances[\"Importance\"] > 0.05][\n    \"Feature\"\n].tolist()\n\n# Prepare the data\nX = heart_diseases.drop(\n    columns=[\"num\"]\n)  # Drop columns 'num' and 'id' from the dataset\ny = heart_diseases[\"num\"]  # Target variable 'num'\n\n# Create the model\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nscores = cross_val_score(rf, X[important_features], y, cv=10)\n\nprint(f\"Cross-validation scores: {scores}\")\nprint(f\"Mean cross-validation score: {scores.mean()}\")\ngrid_search.fit(X_train[important_features], y_train)\n\n# Train the model on the training set\nrf.fit(X_train[important_features], y_train)\n\n# Make predictions on the test set\ny_pred = rf.predict(X_test[important_features])\n# Print feature importance columns\nprint(f\"Features used in the prediction: {important_features}\")\nprint(f\"important_features = {important_features}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Cross-validation scores: [0.9        0.83333333 0.9        0.9        0.8        0.66666667\n 0.7        0.8        0.73333333 0.93103448]\nMean cross-validation score: 0.8164367816091953\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\nFeatures used in the prediction: ['cp_asymptomatic', 'ca', 'oldpeak', 'Systolic blood pressure', 'thal_normal', 'thal_reversable defect', 'thalch', 'age']\nimportant_features = ['cp_asymptomatic', 'ca', 'oldpeak', 'Systolic blood pressure', 'thal_normal', 'thal_reversable defect', 'thalch', 'age']\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n)\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Example data setup (replace with your data)\n# X, y = <your_features>, <your_labels>\n# feature_importances = <your_feature_importances_dataframe>\n\nstart_time = time.time()\n\n# Filter features based on importance\nselected_features = feature_importances[feature_importances[\"Importance\"] > 0.05][\n    \"Feature\"\n].tolist()\nX_train, X_test, y_train, y_test = train_test_split(\n    X[selected_features], y, test_size=0.2, random_state=42\n)\nprint(\"Selected Features:\", X_train.columns)\n\n# Define the parameter grid for GridSearchCV\nparam_grid = {\n    \"criterion\": [\"gini\"],\n    \"max_depth\": [6],\n    \"max_features\": [\"auto\"],\n    \"n_estimators\": [200],\n}\n\n\n# Evaluate on the test set\n# Fit the RandomForestClassifier\nrf = RandomForestClassifier(**best_params)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nruntime = time.time() - start_time\ntest_accuracy = accuracy_score(y_test, y_pred)\ntest_precision = precision_score(y_test, y_pred)\ntest_recall = recall_score(y_test, y_pred)\ntest_conf_matrix = confusion_matrix(y_test, y_pred)\n\n# Evaluate on the training set\ny_train_pred = rf.predict(X_train)\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntrain_precision = precision_score(y_train, y_train_pred)\ntrain_recall = recall_score(y_train, y_train_pred)\ntrain_conf_matrix = confusion_matrix(y_train, y_train_pred)\n\n# Print performance metrics\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")\nprint(f\"Test Confusion Matrix:\\n{test_conf_matrix}\")\nprint(f\"Train Accuracy: {train_accuracy}\")\nprint(\n    f\"Train Precision: {precision_score(y_train, rf.predict(X_train), average='binary')}\"\n)\nprint(f\"Train Recall: {train_recall}\")\nprint(f\"Train Confusion Matrix:\\n{train_conf_matrix}\")\nprint(f\"Runtime: {runtime:.2f} seconds\")\n\n# Calculate total number of nodes, leaves, and root nodes\nn_nodes = 0\nn_leaves = 0\nn_root_nodes = len(rf.estimators_)  # Each tree has one root node\n\nfor tree in rf.estimators_:\n    n_nodes += tree.tree_.node_count  # Total nodes in the tree\n    n_leaves += tree.tree_.n_leaves  # Total leaves in the tree\n\n# Print total number of nodes and leaves\nprint(f\"Total Number of Nodes: {n_nodes}\")\nprint(f\"Total Number of Leaves: {n_leaves}\")\nprint(f\"Number of Root Nodes: {n_root_nodes}\")","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Selected Features: Index(['cp_asymptomatic', 'ca', 'oldpeak', 'Systolic blood pressure',\n       'thal_normal', 'thal_reversable defect', 'thalch', 'age'],\n      dtype='object')\n/home/hexuser/.cache/pypoetry/virtualenvs/python-kernel-OtKFaj5M-py3.9/lib/python3.9/site-packages/sklearn/ensemble/_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n  warn(\nTest Accuracy: 0.9\nTest Precision: 0.9130434782608695\nTest Recall: 0.84\nTest Confusion Matrix:\n[[33  2]\n [ 4 21]]\nTrain Accuracy: 0.9497907949790795\nTrain Precision: 0.9811320754716981\nTrain Recall: 0.9122807017543859\nTrain Confusion Matrix:\n[[123   2]\n [ 10 104]]\nRuntime: 0.36 seconds\nTotal Number of Nodes: 12064\nTotal Number of Leaves: 6132\nNumber of Root Nodes: 200\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","source":"from sklearn.linear_model import Perceptron\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nX = heart_diseases_2.drop(columns=[\"num\"])\ny = heart_diseases_2[\"num\"]\n# Create Perceptron model\nperceptron = Perceptron()\n\n# Train the model\nperceptron.fit(X_train, y_train)\n\n# Make predictions\ny_train_pred = perceptron.predict(X_train)\ny_test_pred = perceptron.predict(X_test)\n\n# Evaluate the model\ntrain_accuracy = accuracy_score(y_train, y_train_pred)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntrain_precision = precision_score(y_train, y_train_pred)\ntest_precision = precision_score(y_test, y_test_pred)\ntrain_recall = recall_score(y_train, y_train_pred)\ntest_recall = recall_score(y_test, y_test_pred)\n\nprint(\"Train Accuracy:\", train_accuracy)\nprint(\"Test Accuracy:\", test_accuracy)\nprint(\"Train Precision:\", train_precision)\nprint(\"Test Precision:\", test_precision)\nprint(\"Train Recall:\", train_recall)\nprint(\"Test Recall:\", test_recall)","metadata":{},"execution_count":null,"outputs":[{"data":{"text/plain":"Train Accuracy: 0.5271966527196653\nTest Accuracy: 0.6\nTrain Precision: 1.0\nTest Precision: 1.0\nTrain Recall: 0.008771929824561403\nTest Recall: 0.04\n"},"execution_count":null,"metadata":{},"output_type":"execute_result"}]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Matt Rothman","project_id":"5201d03d-79a9-4f43-82b3-8a89c43ad1f3","version":"draft","exported_date":"Sun Dec 15 2024 20:25:04 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}