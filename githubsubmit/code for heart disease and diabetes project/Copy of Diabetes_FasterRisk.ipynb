{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/msr216/.conda/envs/fasterrisk/lib/python39.zip', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9/lib-dynload', '', '/home/msr216/.conda/envs/fasterrisk/lib/python3.9/site-packages']\n",
      "Requirement already satisfied: fasterrisk in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (0.1.10)\n",
      "Requirement already satisfied: matplotlib==3.7.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (3.7.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.3 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.26.4)\n",
      "Requirement already satisfied: pandas==1.5.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.5.2)\n",
      "Requirement already satisfied: pillow==9.4.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (9.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn==1.2.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from fasterrisk) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (24.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from matplotlib==3.7.2->fasterrisk) (6.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from pandas==1.5.2->fasterrisk) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from scikit-learn==1.2.0->fasterrisk) (3.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from requests<3.0.0,>=2.28.1->fasterrisk) (2024.8.30)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.7.2->fasterrisk) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/envs/fasterrisk/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.7.2->fasterrisk) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "\n",
    "\n",
    "!pip install fasterrisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, confusion_matrix, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in the last column: 2\n",
      "Number of unique values in the last column: 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"diabetes_train.csv\")\n",
    "unique_count = df.iloc[:, -1].nunique()\n",
    "print(f\"Number of unique values in the last column: {unique_count}\")\n",
    "\n",
    "df = pd.read_csv(\"diabetes_test.csv\")\n",
    "unique_count = df.iloc[:, -1].nunique()\n",
    "print(f\"Number of unique values in the last column: {unique_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Glucose_0  Glucose_1  BloodPressure_0  BloodPressure_1  SkinThickness_0  \\\n",
      "0            1          0                1                0                1   \n",
      "1            1          0                0                1                0   \n",
      "2            0          1                1                0                1   \n",
      "3            0          1                1                0                1   \n",
      "4            0          1                0                1                0   \n",
      "..         ...        ...              ...              ...              ...   \n",
      "609          0          1                1                0                0   \n",
      "610          1          0                0                1                1   \n",
      "611          1          0                0                1                0   \n",
      "612          0          1                1                0                1   \n",
      "613          0          1                0                1                1   \n",
      "\n",
      "     SkinThickness_1  Insulin_0  Insulin_1  BMI_0  BMI_1  ...  \\\n",
      "0                  0          1          0      1      0  ...   \n",
      "1                  1          1          0      1      0  ...   \n",
      "2                  0          0          1      1      0  ...   \n",
      "3                  0          1          0      1      0  ...   \n",
      "4                  1          0          1      0      1  ...   \n",
      "..               ...        ...        ...    ...    ...  ...   \n",
      "609                1          0          1      1      0  ...   \n",
      "610                0          1          0      1      0  ...   \n",
      "611                1          1          0      0      1  ...   \n",
      "612                0          1          0      0      1  ...   \n",
      "613                0          1          0      1      0  ...   \n",
      "\n",
      "     Pregnancies_12_0  Pregnancies_12_1  Pregnancies_13_0  Pregnancies_13_1  \\\n",
      "0                   1                 0                 1                 0   \n",
      "1                   1                 0                 1                 0   \n",
      "2                   1                 0                 1                 0   \n",
      "3                   1                 0                 1                 0   \n",
      "4                   1                 0                 1                 0   \n",
      "..                ...               ...               ...               ...   \n",
      "609                 1                 0                 1                 0   \n",
      "610                 1                 0                 1                 0   \n",
      "611                 1                 0                 1                 0   \n",
      "612                 1                 0                 1                 0   \n",
      "613                 1                 0                 1                 0   \n",
      "\n",
      "     Pregnancies_14_0  Pregnancies_14_1  Pregnancies_15_0  Pregnancies_15_1  \\\n",
      "0                   1                 0                 1                 0   \n",
      "1                   1                 0                 1                 0   \n",
      "2                   1                 0                 1                 0   \n",
      "3                   1                 0                 1                 0   \n",
      "4                   1                 0                 1                 0   \n",
      "..                ...               ...               ...               ...   \n",
      "609                 1                 0                 1                 0   \n",
      "610                 1                 0                 1                 0   \n",
      "611                 1                 0                 1                 0   \n",
      "612                 1                 0                 1                 0   \n",
      "613                 1                 0                 1                 0   \n",
      "\n",
      "     Pregnancies_17_0  Pregnancies_17_1  \n",
      "0                   1                 0  \n",
      "1                   1                 0  \n",
      "2                   1                 0  \n",
      "3                   1                 0  \n",
      "4                   1                 0  \n",
      "..                ...               ...  \n",
      "609                 1                 0  \n",
      "610                 1                 0  \n",
      "611                 1                 0  \n",
      "612                 1                 0  \n",
      "613                 1                 0  \n",
      "\n",
      "[614 rows x 68 columns]\n",
      "     Glucose_0  Glucose_1  BloodPressure_0  BloodPressure_1  SkinThickness_0  \\\n",
      "0           -1         -1               -1               -1               -1   \n",
      "1           -1         -1               -1               -1               -1   \n",
      "2           -1         -1               -1               -1               -1   \n",
      "3           -1         -1               -1               -1               -1   \n",
      "4           -1         -1               -1               -1               -1   \n",
      "..         ...        ...              ...              ...              ...   \n",
      "609         -1         -1               -1               -1               -1   \n",
      "610         -1         -1               -1               -1               -1   \n",
      "611         -1         -1               -1               -1               -1   \n",
      "612         -1         -1               -1               -1               -1   \n",
      "613         -1         -1               -1               -1               -1   \n",
      "\n",
      "     SkinThickness_1  Insulin_0  Insulin_1  BMI_0  BMI_1  ...  \\\n",
      "0                 -1         -1         -1     -1     -1  ...   \n",
      "1                 -1         -1         -1     -1     -1  ...   \n",
      "2                 -1         -1         -1     -1     -1  ...   \n",
      "3                 -1         -1         -1     -1     -1  ...   \n",
      "4                 -1         -1         -1     -1     -1  ...   \n",
      "..               ...        ...        ...    ...    ...  ...   \n",
      "609               -1         -1         -1     -1     -1  ...   \n",
      "610               -1         -1         -1     -1     -1  ...   \n",
      "611               -1         -1         -1     -1     -1  ...   \n",
      "612               -1         -1         -1     -1     -1  ...   \n",
      "613               -1         -1         -1     -1     -1  ...   \n",
      "\n",
      "     Pregnancies_12_1  Pregnancies_13_0  Pregnancies_13_1  Pregnancies_14_0  \\\n",
      "0                  -1                -1                -1                -1   \n",
      "1                  -1                -1                -1                -1   \n",
      "2                  -1                -1                -1                -1   \n",
      "3                  -1                -1                -1                -1   \n",
      "4                  -1                -1                -1                -1   \n",
      "..                ...               ...               ...               ...   \n",
      "609                -1                -1                -1                -1   \n",
      "610                -1                -1                -1                -1   \n",
      "611                -1                -1                -1                -1   \n",
      "612                -1                -1                -1                -1   \n",
      "613                -1                -1                -1                -1   \n",
      "\n",
      "     Pregnancies_14_1  Pregnancies_15_0  Pregnancies_15_1  Pregnancies_17_0  \\\n",
      "0                  -1                -1                -1                -1   \n",
      "1                  -1                -1                -1                -1   \n",
      "2                  -1                -1                -1                -1   \n",
      "3                  -1                -1                -1                -1   \n",
      "4                  -1                -1                -1                -1   \n",
      "..                ...               ...               ...               ...   \n",
      "609                -1                -1                -1                -1   \n",
      "610                -1                -1                -1                -1   \n",
      "611                -1                -1                -1                -1   \n",
      "612                -1                -1                -1                -1   \n",
      "613                -1                -1                -1                -1   \n",
      "\n",
      "     Pregnancies_17_1  Outcome_1  \n",
      "0                  -1         -1  \n",
      "1                  -1         -1  \n",
      "2                  -1         -1  \n",
      "3                  -1         -1  \n",
      "4                  -1         -1  \n",
      "..                ...        ...  \n",
      "609                -1         -1  \n",
      "610                -1         -1  \n",
      "611                -1         -1  \n",
      "612                -1         -1  \n",
      "613                -1         -1  \n",
      "\n",
      "[614 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"diabetes_train.csv\")\n",
    "output = df['Outcome_1']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "print(df)\n",
    "df = pd.concat([df,output],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[0],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0]],-1,inplace=True)\n",
    "h = df.columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test DataFrame (before concatenation):\n",
      "     Glucose_0  Glucose_1  BloodPressure_0  BloodPressure_1  SkinThickness_0  \\\n",
      "0            1          0                1                0                0   \n",
      "1            1          0                0                1                0   \n",
      "2            1          0                1                0                1   \n",
      "3            1          0                0                1                1   \n",
      "4            0          1                0                1                1   \n",
      "..         ...        ...              ...              ...              ...   \n",
      "149          0          1                0                1                1   \n",
      "150          1          0                1                0                0   \n",
      "151          1          0                0                1                1   \n",
      "152          0          1                0                1                0   \n",
      "153          1          0                0                1                0   \n",
      "\n",
      "     SkinThickness_1  Insulin_0  Insulin_1  DiabetesPedigreeFunction_0  \\\n",
      "0                  1          0          1                           1   \n",
      "1                  1          1          0                           1   \n",
      "2                  0          1          0                           1   \n",
      "3                  0          1          0                           0   \n",
      "4                  0          1          0                           1   \n",
      "..               ...        ...        ...                         ...   \n",
      "149                0          1          0                           1   \n",
      "150                1          1          0                           0   \n",
      "151                0          1          0                           0   \n",
      "152                1          0          1                           1   \n",
      "153                1          1          0                           0   \n",
      "\n",
      "     DiabetesPedigreeFunction_1  ...  Pregnancies_10_1  Pregnancies_11_0  \\\n",
      "0                             0  ...                 0                 1   \n",
      "1                             0  ...                 0                 1   \n",
      "2                             0  ...                 0                 1   \n",
      "3                             1  ...                 0                 1   \n",
      "4                             0  ...                 0                 1   \n",
      "..                          ...  ...               ...               ...   \n",
      "149                           0  ...                 0                 1   \n",
      "150                           1  ...                 0                 1   \n",
      "151                           1  ...                 0                 1   \n",
      "152                           0  ...                 0                 1   \n",
      "153                           1  ...                 0                 1   \n",
      "\n",
      "     Pregnancies_11_1  Pregnancies_12_0  Pregnancies_12_1  Pregnancies_13_0  \\\n",
      "0                   0                 1                 0                 1   \n",
      "1                   0                 1                 0                 1   \n",
      "2                   0                 1                 0                 1   \n",
      "3                   0                 1                 0                 1   \n",
      "4                   0                 1                 0                 1   \n",
      "..                ...               ...               ...               ...   \n",
      "149                 0                 1                 0                 1   \n",
      "150                 0                 1                 0                 1   \n",
      "151                 0                 1                 0                 1   \n",
      "152                 0                 1                 0                 1   \n",
      "153                 0                 1                 0                 1   \n",
      "\n",
      "     Pregnancies_13_1  Pregnancies_14_0  Pregnancies_15_0  Pregnancies_17_0  \n",
      "0                   0                 1                 1                 1  \n",
      "1                   0                 1                 1                 1  \n",
      "2                   0                 1                 1                 1  \n",
      "3                   0                 1                 1                 1  \n",
      "4                   0                 1                 1                 1  \n",
      "..                ...               ...               ...               ...  \n",
      "149                 0                 1                 1                 1  \n",
      "150                 0                 1                 1                 1  \n",
      "151                 0                 1                 1                 1  \n",
      "152                 0                 1                 1                 1  \n",
      "153                 0                 1                 1                 1  \n",
      "\n",
      "[154 rows x 65 columns]\n",
      "Final Processed Test DataFrame:\n",
      "     Glucose_0  Glucose_1  BloodPressure_0  BloodPressure_1  SkinThickness_0  \\\n",
      "0           -1         -1               -1               -1               -1   \n",
      "1           -1         -1               -1               -1               -1   \n",
      "2           -1         -1               -1               -1               -1   \n",
      "3           -1         -1               -1               -1               -1   \n",
      "4           -1         -1               -1               -1               -1   \n",
      "..         ...        ...              ...              ...              ...   \n",
      "149         -1         -1               -1               -1               -1   \n",
      "150         -1         -1               -1               -1               -1   \n",
      "151         -1         -1               -1               -1               -1   \n",
      "152         -1         -1               -1               -1               -1   \n",
      "153         -1         -1               -1               -1               -1   \n",
      "\n",
      "     SkinThickness_1  Insulin_0  Insulin_1  BMI_0  BMI_1  ...  \\\n",
      "0                 -1         -1         -1      0      0  ...   \n",
      "1                 -1         -1         -1      0      0  ...   \n",
      "2                 -1         -1         -1      0      0  ...   \n",
      "3                 -1         -1         -1      0      0  ...   \n",
      "4                 -1         -1         -1      0      0  ...   \n",
      "..               ...        ...        ...    ...    ...  ...   \n",
      "149               -1         -1         -1      0      0  ...   \n",
      "150               -1         -1         -1      0      0  ...   \n",
      "151               -1         -1         -1      0      0  ...   \n",
      "152               -1         -1         -1      0      0  ...   \n",
      "153               -1         -1         -1      0      0  ...   \n",
      "\n",
      "     Pregnancies_12_1  Pregnancies_13_0  Pregnancies_13_1  Pregnancies_14_0  \\\n",
      "0                  -1                -1                -1                -1   \n",
      "1                  -1                -1                -1                -1   \n",
      "2                  -1                -1                -1                -1   \n",
      "3                  -1                -1                -1                -1   \n",
      "4                  -1                -1                -1                -1   \n",
      "..                ...               ...               ...               ...   \n",
      "149                -1                -1                -1                -1   \n",
      "150                -1                -1                -1                -1   \n",
      "151                -1                -1                -1                -1   \n",
      "152                -1                -1                -1                -1   \n",
      "153                -1                -1                -1                -1   \n",
      "\n",
      "     Pregnancies_14_1  Pregnancies_15_0  Pregnancies_15_1  Pregnancies_17_0  \\\n",
      "0                   0                -1                 0                -1   \n",
      "1                   0                -1                 0                -1   \n",
      "2                   0                -1                 0                -1   \n",
      "3                   0                -1                 0                -1   \n",
      "4                   0                -1                 0                -1   \n",
      "..                ...               ...               ...               ...   \n",
      "149                 0                -1                 0                -1   \n",
      "150                 0                -1                 0                -1   \n",
      "151                 0                -1                 0                -1   \n",
      "152                 0                -1                 0                -1   \n",
      "153                 0                -1                 0                -1   \n",
      "\n",
      "     Pregnancies_17_1  Outcome_1  \n",
      "0                   0         -1  \n",
      "1                   0         -1  \n",
      "2                   0         -1  \n",
      "3                   0         -1  \n",
      "4                   0         -1  \n",
      "..                ...        ...  \n",
      "149                 0         -1  \n",
      "150                 0         -1  \n",
      "151                 0         -1  \n",
      "152                 0         -1  \n",
      "153                 0         -1  \n",
      "\n",
      "[154 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the test dataset\n",
    "df2 = pd.read_csv(\"diabetes_test.csv\")\n",
    "output = df2['Outcome_1']  # Store the output column\n",
    "df2 = df2.iloc[:, :-1]  # Drop the output column from features\n",
    "\n",
    "# One-hot encoding for the test data\n",
    "df2 = pd.get_dummies(df2, columns=df2.columns)\n",
    "print(\"Processed Test DataFrame (before concatenation):\")\n",
    "print(df2)\n",
    "\n",
    "# Add the output column back to the test dataframe\n",
    "df2['Outcome_1'] = output\n",
    "\n",
    "# Replace output values with 1 and -1\n",
    "unique_test = df2['Outcome_1'].unique()\n",
    "if len(unique_test) > 1:\n",
    "    df2.replace(unique_test[0], 1, inplace=True)\n",
    "    df2.replace(unique_test[1], -1, inplace=True)\n",
    "elif len(unique_test) == 1:\n",
    "    df2['output'] = 1 if unique_test[0] == 1 else -1  # Adjust as needed\n",
    "\n",
    "# Ensure the test DataFrame has the same columns as the training DataFrame\n",
    "# Assuming 'h' is defined from the training DataFrame earlier\n",
    "df2 = df2.reindex(columns=h, fill_value=0)\n",
    "\n",
    "# Final output\n",
    "print(\"Final Processed Test DataFrame:\")\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Training DataFrame:\n",
      "     Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n",
      "0          0              0              0        0    0   \n",
      "1          0              1              1        0    0   \n",
      "2          1              0              0        1    0   \n",
      "3          1              0              0        0    0   \n",
      "4          1              1              1        1    1   \n",
      "..       ...            ...            ...      ...  ...   \n",
      "609        1              0              1        1    0   \n",
      "610        0              1              0        0    0   \n",
      "611        0              1              1        0    1   \n",
      "612        1              0              0        0    1   \n",
      "613        1              1              0        0    0   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  hypertension  heart_disease  HbA1c_level  \\\n",
      "0                           0    0             0              0            1   \n",
      "1                           1    1             0              0            1   \n",
      "2                           1    0             0              0            0   \n",
      "3                           0    1             0              0            1   \n",
      "4                           0    1             0              0            0   \n",
      "..                        ...  ...           ...            ...          ...   \n",
      "609                         0    0             0              0            1   \n",
      "610                         0    0             0              0            0   \n",
      "611                         1    1             0              0            0   \n",
      "612                         0    0             0              0            0   \n",
      "613                         0    0             0              0            1   \n",
      "\n",
      "     ...  Pregnancies_7  Pregnancies_8  Pregnancies_9  Pregnancies_10  \\\n",
      "0    ...              0              0              0               0   \n",
      "1    ...              0              0              1               0   \n",
      "2    ...              0              0              0               0   \n",
      "3    ...              0              0              0               0   \n",
      "4    ...              0              0              0               0   \n",
      "..   ...            ...            ...            ...             ...   \n",
      "609  ...              0              0              0               0   \n",
      "610  ...              0              0              0               0   \n",
      "611  ...              0              0              0               1   \n",
      "612  ...              0              0              0               0   \n",
      "613  ...              0              0              0               0   \n",
      "\n",
      "     Pregnancies_11  Pregnancies_12  Pregnancies_13  Pregnancies_14  \\\n",
      "0                 0               0               0               0   \n",
      "1                 0               0               0               0   \n",
      "2                 0               0               0               0   \n",
      "3                 0               0               0               0   \n",
      "4                 0               0               0               0   \n",
      "..              ...             ...             ...             ...   \n",
      "609               0               0               0               0   \n",
      "610               0               0               0               0   \n",
      "611               0               0               0               0   \n",
      "612               0               0               0               0   \n",
      "613               0               0               0               0   \n",
      "\n",
      "     Pregnancies_15  Pregnancies_17  \n",
      "0                 0               0  \n",
      "1                 0               0  \n",
      "2                 0               0  \n",
      "3                 0               0  \n",
      "4                 0               0  \n",
      "..              ...             ...  \n",
      "609               0               0  \n",
      "610               0               0  \n",
      "611               0               0  \n",
      "612               0               0  \n",
      "613               0               0  \n",
      "\n",
      "[614 rows x 34 columns]\n",
      "Processed Test DataFrame (before concatenation):\n",
      "     Glucose  BloodPressure  SkinThickness  Insulin  DiabetesPedigreeFunction  \\\n",
      "0          0              0              1        1                         0   \n",
      "1          0              1              1        0                         0   \n",
      "2          0              0              0        0                         0   \n",
      "3          0              1              0        0                         1   \n",
      "4          1              1              0        0                         0   \n",
      "..       ...            ...            ...      ...                       ...   \n",
      "149        1              1              0        0                         0   \n",
      "150        0              0              1        0                         1   \n",
      "151        0              1              0        0                         1   \n",
      "152        1              1              1        1                         0   \n",
      "153        0              1              1        0                         1   \n",
      "\n",
      "     age_1  hypertension  heart_disease  bmi_1  HbA1c_level  ...  \\\n",
      "0        1             0              0      0            0  ...   \n",
      "1        0             0              0      0            0  ...   \n",
      "2        1             0              0      1            1  ...   \n",
      "3        1             0              0      0            0  ...   \n",
      "4        0             0              0      1            1  ...   \n",
      "..     ...           ...            ...    ...          ...  ...   \n",
      "149      0             0              0      0            0  ...   \n",
      "150      0             0              0      1            1  ...   \n",
      "151      1             0              0      1            1  ...   \n",
      "152      0             0              0      0            1  ...   \n",
      "153      0             0              0      0            1  ...   \n",
      "\n",
      "     Pregnancies_7  Pregnancies_8  Pregnancies_9  Pregnancies_10  \\\n",
      "0                0              0              0               0   \n",
      "1                0              0              0               0   \n",
      "2                0              0              0               0   \n",
      "3                0              1              0               0   \n",
      "4                1              0              0               0   \n",
      "..             ...            ...            ...             ...   \n",
      "149              0              0              1               0   \n",
      "150              0              0              0               0   \n",
      "151              0              1              0               0   \n",
      "152              0              0              0               0   \n",
      "153              0              1              0               0   \n",
      "\n",
      "     Pregnancies_11  Pregnancies_12  Pregnancies_13  Pregnancies_14  \\\n",
      "0                 0               0               0               0   \n",
      "1                 0               0               0               0   \n",
      "2                 0               0               0               0   \n",
      "3                 0               0               0               0   \n",
      "4                 0               0               0               0   \n",
      "..              ...             ...             ...             ...   \n",
      "149               0               0               0               0   \n",
      "150               0               0               0               0   \n",
      "151               0               0               0               0   \n",
      "152               0               0               0               0   \n",
      "153               0               0               0               0   \n",
      "\n",
      "     Pregnancies_15  Pregnancies_17  \n",
      "0                 0               0  \n",
      "1                 0               0  \n",
      "2                 0               0  \n",
      "3                 0               0  \n",
      "4                 0               0  \n",
      "..              ...             ...  \n",
      "149               0               0  \n",
      "150               0               0  \n",
      "151               0               0  \n",
      "152               0               0  \n",
      "153               0               0  \n",
      "\n",
      "[154 rows x 34 columns]\n",
      "Final Processed Test DataFrame:\n",
      "     Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n",
      "0          0              0              1        1    0   \n",
      "1          0              1              1        0    0   \n",
      "2          0              0              0        0    0   \n",
      "3          0              1              0        0    0   \n",
      "4          1              1              0        0    0   \n",
      "..       ...            ...            ...      ...  ...   \n",
      "149        1              1              0        0    0   \n",
      "150        0              0              1        0    0   \n",
      "151        0              1              0        0    0   \n",
      "152        1              1              1        1    0   \n",
      "153        0              1              1        0    0   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  hypertension  heart_disease  HbA1c_level  \\\n",
      "0                           0    0             0              0            0   \n",
      "1                           0    0             0              0            0   \n",
      "2                           0    0             0              0            1   \n",
      "3                           1    0             0              0            0   \n",
      "4                           0    0             0              0            1   \n",
      "..                        ...  ...           ...            ...          ...   \n",
      "149                         0    0             0              0            0   \n",
      "150                         1    0             0              0            1   \n",
      "151                         1    0             0              0            1   \n",
      "152                         0    0             0              0            1   \n",
      "153                         1    0             0              0            1   \n",
      "\n",
      "     ...  Pregnancies_8  Pregnancies_9  Pregnancies_10  Pregnancies_11  \\\n",
      "0    ...              0              0               0               0   \n",
      "1    ...              0              0               0               0   \n",
      "2    ...              0              0               0               0   \n",
      "3    ...              1              0               0               0   \n",
      "4    ...              0              0               0               0   \n",
      "..   ...            ...            ...             ...             ...   \n",
      "149  ...              0              1               0               0   \n",
      "150  ...              0              0               0               0   \n",
      "151  ...              1              0               0               0   \n",
      "152  ...              0              0               0               0   \n",
      "153  ...              1              0               0               0   \n",
      "\n",
      "     Pregnancies_12  Pregnancies_13  Pregnancies_14  Pregnancies_15  \\\n",
      "0                 0               0               0               0   \n",
      "1                 0               0               0               0   \n",
      "2                 0               0               0               0   \n",
      "3                 0               0               0               0   \n",
      "4                 0               0               0               0   \n",
      "..              ...             ...             ...             ...   \n",
      "149               0               0               0               0   \n",
      "150               0               0               0               0   \n",
      "151               0               0               0               0   \n",
      "152               0               0               0               0   \n",
      "153               0               0               0               0   \n",
      "\n",
      "     Pregnancies_17  Outcome_1  \n",
      "0                 0         -1  \n",
      "1                 0         -1  \n",
      "2                 0         -1  \n",
      "3                 0         -1  \n",
      "4                 0         -1  \n",
      "..              ...        ...  \n",
      "149               0          1  \n",
      "150               0         -1  \n",
      "151               0         -1  \n",
      "152               0          1  \n",
      "153               0         -1  \n",
      "\n",
      "[154 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset\n",
    "df = pd.read_csv(\"diabetes_train.csv\")\n",
    "output = df['Outcome_1']\n",
    "df = df.iloc[:, :-1]  # Drop the output column from features\n",
    "\n",
    "# Print the original data (already one-hot encoded)\n",
    "print(\"Processed Training DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Add the output column back to the training DataFrame\n",
    "df['Outcome_1'] = output\n",
    "\n",
    "# Replace output values (fix the column name to 'Outcome_1' here)\n",
    "df['Outcome_1'].replace(0, -1, inplace=True)\n",
    "df['Outcome_1'].replace(1, 1, inplace=True)\n",
    "\n",
    "# Store the column names for later use\n",
    "h = df.columns\n",
    "\n",
    "# Load the test dataset\n",
    "df2 = pd.read_csv(\"diabetes_test.csv\")\n",
    "output2 = df2['Outcome_1']\n",
    "df2 = df2.iloc[:, :-1]  # Drop the output column from features\n",
    "\n",
    "# Print the original test data (already one-hot encoded)\n",
    "print(\"Processed Test DataFrame (before concatenation):\")\n",
    "print(df2)\n",
    "\n",
    "# Add the output column back to the test DataFrame\n",
    "df2['Outcome_1'] = output2\n",
    "\n",
    "# Replace output values (fix the column name to 'Outcome_1' here)\n",
    "df2['Outcome_1'].replace(0, -1, inplace=True)\n",
    "df2['Outcome_1'].replace(1, 1, inplace=True)\n",
    "\n",
    "# Ensure the test DataFrame has the same columns as the training DataFrame\n",
    "df2 = df2.reindex(columns=h, fill_value=0)\n",
    "\n",
    "# Final output\n",
    "print(\"Final Processed Test DataFrame:\")\n",
    "print(df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Outcome_1'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Outcome_1'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.744299674267101\n",
      "AUC:  0.7951834029948602\n",
      "Precision:  0.7363658257280751\n",
      "Recall:  0.744299674267101\n",
      "Total Run Time: 4.0259 seconds\n",
      "Risk Score Model Coefficients:\n",
      "Multiplier:  1.0\n",
      "Intercept:  -3.0\n",
      "Coefficients:  [ 2.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. -1.  5.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Start the total runtime timer\n",
    "start_time = time.time()\n",
    "\n",
    "sparsity = 10  # produce a risk score model with 10 nonzero coefficients\n",
    "\n",
    "# Initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n",
    "\n",
    "# Perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# Get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models()\n",
    "\n",
    "# Get the first solution from the final diverse pool\n",
    "multiplier, intercept, coefficients = m.get_models(model_index=0)\n",
    "\n",
    "# Feature names\n",
    "X_featureNames = df.iloc[:, :-1].columns.values.tolist()\n",
    "\n",
    "# Create a classifier\n",
    "clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n",
    "\n",
    "# Get the predicted label\n",
    "y_pred = clf.predict(X=X_train)\n",
    "\n",
    "# Get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X=X_train)\n",
    "\n",
    "# Compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X=X_train, y=y_train)\n",
    "\n",
    "# Get accuracy and area under the ROC curve (AUC)\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X=X_train, y=y_train)\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision_train = precision_score(y_train, y_pred, average='weighted')\n",
    "recall_train = recall_score(y_train, y_pred, average='weighted')\n",
    "\n",
    "# End the total runtime timer\n",
    "end_time = time.time()\n",
    "total_run_time = end_time - start_time\n",
    "\n",
    "# Print the results\n",
    "print(\"Accuracy: \", acc_train)\n",
    "print(\"AUC: \", auc_train)\n",
    "print(\"Precision: \", precision_train)\n",
    "print(\"Recall: \", recall_train)\n",
    "\n",
    "# Print the total runtime\n",
    "print(f\"Total Run Time: {total_run_time:.4f} seconds\")\n",
    "\n",
    "# Optionally, you can print the risk score model information:\n",
    "print(\"Risk Score Model Coefficients:\")\n",
    "print(\"Multiplier: \", multiplier)\n",
    "print(\"Intercept: \", intercept)\n",
    "print(\"Coefficients: \", coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Metrics:\n",
      "Accuracy:  0.6623376623376623\n",
      "AUC:  0.7455463728191001\n",
      "Precision:  0.7085330776605945\n",
      "Recall:  0.6623376623376623\n",
      "\n",
      "Testing Metrics:\n",
      "Accuracy:  0.6623376623376623\n",
      "AUC:  0.7455463728191001\n",
      "Precision:  0.7085330776605945\n",
      "Recall:  0.6623376623376623\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_train_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate confusion matrix for the test set\u001b[39;00m\n\u001b[1;32m     60\u001b[0m cm_test \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_test_pred)\n\u001b[0;32m---> 61\u001b[0m cm_train \u001b[38;5;241m=\u001b[39m confusion_matrix(y_train, \u001b[43my_train_pred\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Print the confusion matrix\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix (Train):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "\n",
    "# Sparsity: Number of non-zero coefficients in the model\n",
    "sparsity = 10  # Produce a risk score model with 10 nonzero coefficients\n",
    "\n",
    "# Initialize a risk score optimizer with training data\n",
    "m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n",
    "\n",
    "# Perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# Get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models()\n",
    "\n",
    "# Get the first solution from the final diverse pool\n",
    "multiplier, intercept, coefficients = m.get_models(model_index=0)\n",
    "\n",
    "# Feature names\n",
    "X_featureNames = df2.iloc[:, :-1].columns.values.tolist()  # Assuming df2 has the same structure\n",
    "\n",
    "# Create a classifier\n",
    "clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n",
    "\n",
    "# Get the predicted label for the test set\n",
    "y_test_pred = clf.predict(X=X_test)\n",
    "\n",
    "# Get the probability of predicting y[i] with label +1 for the test set\n",
    "try:\n",
    "    y_test_pred_prob = clf.predict_prob(X=X_test)\n",
    "except Exception as e:\n",
    "    print(\"Error while predicting probabilities:\", e)\n",
    "    y_test_pred_prob = (intercept + X_test.dot(coefficients)) / multiplier\n",
    "\n",
    "# Compute the logistic loss for the test set\n",
    "logisticLoss_test = clf.compute_logisticLoss(X=X_test, y=y_test)\n",
    "\n",
    "# Get accuracy and area under the ROC curve (AUC) for the test set\n",
    "acc_test, auc_test = clf.get_acc_and_auc(X=X_test, y=y_test)\n",
    "\n",
    "# Calculate precision and recall for the test set\n",
    "precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Print the results for test metrics\n",
    "print(\"Test Metrics:\")\n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"AUC: \", auc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(\"Accuracy: \", acc_test)\n",
    "print(\"AUC: \", auc_test)\n",
    "print(\"Precision: \", precision_test)\n",
    "print(\"Recall: \", recall_test)\n",
    "\n",
    "# Calculate confusion matrix for the test set\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "cm_train = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"\\nConfusion Matrix (Train):\")\n",
    "print(cm_train)\n",
    "\n",
    "print(\"\\nConfusion Matrix (Test):\")\n",
    "print(cm_test)\n",
    "\n",
    "# Print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6623376623376623\n",
      "AUC:  0.7455463728191001\n",
      "Precision:  0.7363658257280751\n",
      "Recall:  0.744299674267101\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \", acc_train)\n",
    "print(\"AUC: \", auc_train)\n",
    "print(\"Precision: \", precision_train)\n",
    "print(\"Recall: \", recall_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6623376623376623 \n",
      "AUC:  0.7455463728191001\n"
     ]
    }
   ],
   "source": [
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  318.05912905806065\n",
      "Test Loss:  85.49035484006396\n"
     ]
    }
   ],
   "source": [
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Test DataFrame (before concatenation):\n",
      "     Glucose_0  Glucose_1  BloodPressure_0  BloodPressure_1  SkinThickness_0  \\\n",
      "0            1          0                1                0                0   \n",
      "1            1          0                0                1                0   \n",
      "2            1          0                1                0                1   \n",
      "3            1          0                0                1                1   \n",
      "4            0          1                0                1                1   \n",
      "..         ...        ...              ...              ...              ...   \n",
      "149          0          1                0                1                1   \n",
      "150          1          0                1                0                0   \n",
      "151          1          0                0                1                1   \n",
      "152          0          1                0                1                0   \n",
      "153          1          0                0                1                0   \n",
      "\n",
      "     SkinThickness_1  Insulin_0  Insulin_1  BMI_0  BMI_1  ...  \\\n",
      "0                  1          0          1      0      1  ...   \n",
      "1                  1          1          0      0      1  ...   \n",
      "2                  0          1          0      1      0  ...   \n",
      "3                  0          1          0      1      0  ...   \n",
      "4                  0          1          0      1      0  ...   \n",
      "..               ...        ...        ...    ...    ...  ...   \n",
      "149                0          1          0      1      0  ...   \n",
      "150                1          1          0      0      1  ...   \n",
      "151                0          1          0      0      1  ...   \n",
      "152                1          0          1      1      0  ...   \n",
      "153                1          1          0      0      1  ...   \n",
      "\n",
      "     Pregnancies_10_1  Pregnancies_11_0  Pregnancies_11_1  Pregnancies_12_0  \\\n",
      "0                   0                 1                 0                 1   \n",
      "1                   0                 1                 0                 1   \n",
      "2                   0                 1                 0                 1   \n",
      "3                   0                 1                 0                 1   \n",
      "4                   0                 1                 0                 1   \n",
      "..                ...               ...               ...               ...   \n",
      "149                 0                 1                 0                 1   \n",
      "150                 0                 1                 0                 1   \n",
      "151                 0                 1                 0                 1   \n",
      "152                 0                 1                 0                 1   \n",
      "153                 0                 1                 0                 1   \n",
      "\n",
      "     Pregnancies_12_1  Pregnancies_13_0  Pregnancies_13_1  Pregnancies_14_0  \\\n",
      "0                   0                 1                 0                 1   \n",
      "1                   0                 1                 0                 1   \n",
      "2                   0                 1                 0                 1   \n",
      "3                   0                 1                 0                 1   \n",
      "4                   0                 1                 0                 1   \n",
      "..                ...               ...               ...               ...   \n",
      "149                 0                 1                 0                 1   \n",
      "150                 0                 1                 0                 1   \n",
      "151                 0                 1                 0                 1   \n",
      "152                 0                 1                 0                 1   \n",
      "153                 0                 1                 0                 1   \n",
      "\n",
      "     Pregnancies_15_0  Pregnancies_17_0  \n",
      "0                   1                 1  \n",
      "1                   1                 1  \n",
      "2                   1                 1  \n",
      "3                   1                 1  \n",
      "4                   1                 1  \n",
      "..                ...               ...  \n",
      "149                 1                 1  \n",
      "150                 1                 1  \n",
      "151                 1                 1  \n",
      "152                 1                 1  \n",
      "153                 1                 1  \n",
      "\n",
      "[154 rows x 73 columns]\n",
      "Unique values in Outcome_1 before mapping: [0 1]\n",
      "Outcome_1 column after mapping:\n",
      "[ 1 -1]\n",
      "Final Processed Test DataFrame:\n",
      "     Glucose  BloodPressure  SkinThickness  Insulin  BMI  \\\n",
      "0          0              0              0        0    0   \n",
      "1          0              0              0        0    0   \n",
      "2          0              0              0        0    0   \n",
      "3          0              0              0        0    0   \n",
      "4          0              0              0        0    0   \n",
      "..       ...            ...            ...      ...  ...   \n",
      "149        0              0              0        0    0   \n",
      "150        0              0              0        0    0   \n",
      "151        0              0              0        0    0   \n",
      "152        0              0              0        0    0   \n",
      "153        0              0              0        0    0   \n",
      "\n",
      "     DiabetesPedigreeFunction  Age  age_1  hypertension  heart_disease  ...  \\\n",
      "0                           0    0      0             0              0  ...   \n",
      "1                           0    0      0             0              0  ...   \n",
      "2                           0    0      0             0              0  ...   \n",
      "3                           0    0      0             0              0  ...   \n",
      "4                           0    0      0             0              0  ...   \n",
      "..                        ...  ...    ...           ...            ...  ...   \n",
      "149                         0    0      0             0              0  ...   \n",
      "150                         0    0      0             0              0  ...   \n",
      "151                         0    0      0             0              0  ...   \n",
      "152                         0    0      0             0              0  ...   \n",
      "153                         0    0      0             0              0  ...   \n",
      "\n",
      "     Pregnancies_8  Pregnancies_9  Pregnancies_10  Pregnancies_11  \\\n",
      "0                0              0               0               0   \n",
      "1                0              0               0               0   \n",
      "2                0              0               0               0   \n",
      "3                0              0               0               0   \n",
      "4                0              0               0               0   \n",
      "..             ...            ...             ...             ...   \n",
      "149              0              0               0               0   \n",
      "150              0              0               0               0   \n",
      "151              0              0               0               0   \n",
      "152              0              0               0               0   \n",
      "153              0              0               0               0   \n",
      "\n",
      "     Pregnancies_12  Pregnancies_13  Pregnancies_14  Pregnancies_15  \\\n",
      "0                 0               0               0               0   \n",
      "1                 0               0               0               0   \n",
      "2                 0               0               0               0   \n",
      "3                 0               0               0               0   \n",
      "4                 0               0               0               0   \n",
      "..              ...             ...             ...             ...   \n",
      "149               0               0               0               0   \n",
      "150               0               0               0               0   \n",
      "151               0               0               0               0   \n",
      "152               0               0               0               0   \n",
      "153               0               0               0               0   \n",
      "\n",
      "     Pregnancies_17  Outcome_1  \n",
      "0                 0          1  \n",
      "1                 0          1  \n",
      "2                 0          1  \n",
      "3                 0          1  \n",
      "4                 0          1  \n",
      "..              ...        ...  \n",
      "149               0         -1  \n",
      "150               0          1  \n",
      "151               0          1  \n",
      "152               0         -1  \n",
      "153               0          1  \n",
      "\n",
      "[154 rows x 39 columns]\n",
      "The Risk Score is:\n",
      "1.                     Glucose      2 point(s) |   ...\n",
      "2.                         BMI      1 point(s) | + ...\n",
      "3.    DiabetesPedigreeFunction      1 point(s) | + ...\n",
      "4.                         Age      1 point(s) | + ...\n",
      "5.        smoking_history_ever     -1 point(s) | + ...\n",
      "6.               Pregnancies_7      1 point(s) | + ...\n",
      "7.               Pregnancies_8      1 point(s) | + ...\n",
      "8.               Pregnancies_9      1 point(s) | + ...\n",
      "9.              Pregnancies_14      5 point(s) | + ...\n",
      "                                         SCORE | =    \n",
      "SCORE |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |\n",
      "RISK  |   1.8% |   4.7% |  11.9% |  26.9% |  50.0% |  73.1% |  88.1% |  95.3% |\n",
      "SCORE |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |\n",
      "RISK  |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% | 100.0% | 100.0% |\n",
      "Test Loss:  304.48245214235624\n",
      "Test Accuracy:  0.35714285714285715 \n",
      "Test AUC:  0.5\n",
      "Test Precision:  0.12755102040816327\n",
      "Test Recall:  0.35714285714285715\n",
      "\n",
      "Confusion Matrix (Test):\n",
      "[[55  0]\n",
      " [99  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msr216/.conda/envs/fasterrisk/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type float which has no callable exp method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'exp'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Predictions for test data\u001b[39;00m\n\u001b[1;32m     61\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X\u001b[38;5;241m=\u001b[39mX_test)\n\u001b[0;32m---> 62\u001b[0m y_pred_prob_test \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Compute test loss (optional)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m logisticLoss_test \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mcompute_logisticLoss(X\u001b[38;5;241m=\u001b[39mX_test, y\u001b[38;5;241m=\u001b[39my_test)\n",
      "File \u001b[0;32m~/.conda/envs/fasterrisk/lib/python3.9/site-packages/fasterrisk/fasterrisk.py:276\u001b[0m, in \u001b[0;36mRiskScoreClassifier.predict_prob\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the risk probabilities of predicting each sample y_i with label +1\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m    (1D array with `float` type) probabilities of each sample y_i to be +1 with shape (n, )\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    275\u001b[0m y_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept \u001b[38;5;241m+\u001b[39m X\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefficients)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiplier \u001b[38;5;66;03m# numpy dot.() has some floating point error issues, so we avoid using self.scaled_intercept and self.scaled_coefficients directly\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43my_score\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred_prob\n",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type float which has no callable exp method"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "# Load the test dataset\n",
    "df = pd.read_csv(\"diabetes_test.csv\")\n",
    "primary = df['Outcome_1']  # assuming 'Outcome_1' is the target variable\n",
    "df = df.iloc[:, :-1]  # drop the target variable column\n",
    "df = pd.get_dummies(df, columns=df.columns)  # one-hot encoding of features\n",
    "df = pd.concat([df, primary], axis=1)\n",
    "\n",
    "# Replace target values\n",
    "df.replace(df.iloc[:, -1].unique()[1], 1, inplace=True)  # assuming 1 is the positive class\n",
    "df.replace(df.iloc[:, -1].unique()[0], -1, inplace=True)  # assuming 0 is the negative class\n",
    "\n",
    "# Load the validation/test set\n",
    "df2 = pd.read_csv(\"diabetes_test.csv\")\n",
    "primary2 = df2['Outcome_1']  # same target variable as in df\n",
    "df2 = df2.iloc[:, :-1]  # drop the target variable column\n",
    "df2 = pd.get_dummies(df2, columns=df2.columns)\n",
    "df2 = pd.concat([df2, primary2], axis=1)\n",
    "\n",
    "# Replace target values in test set\n",
    "df2.replace(df2.iloc[:, -1].unique()[1], 1, inplace=True)  # assuming 1 is the positive class\n",
    "df2.replace(df2.iloc[:, -1].unique()[0], -1, inplace=True)  # assuming 0 is the negative class\n",
    "\n",
    "# Handle missing values (if necessary)\n",
    "empty = pd.DataFrame(columns=df.columns)  # assuming columns 'h' exist\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(df.columns)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "# Prepare features and target variables for training and testing\n",
    "X_train = df.iloc[:, :-1].to_numpy()\n",
    "y_train = df['Outcome_1'].to_numpy()\n",
    "X_test = df2.iloc[:, :-1].to_numpy()\n",
    "y_test = df2['Outcome_1'].to_numpy()\n",
    "\n",
    "# Sparsity setting\n",
    "sparsity = 10  # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# Initialize and optimize the model\n",
    "m = RiskScoreOptimizer(X=X_train, y=y_train, k=sparsity)\n",
    "m.optimize()\n",
    "\n",
    "# Get the first solution from the optimization pool\n",
    "multiplier, intercept, coefficients = m.get_models(model_index=0)\n",
    "X_featureNames = df.iloc[:, :-1].columns.values.tolist()\n",
    "\n",
    "# Create classifier\n",
    "clf = RiskScoreClassifier(multiplier=multiplier, intercept=intercept, coefficients=coefficients, featureNames=X_featureNames)\n",
    "\n",
    "# Predictions for training data (not needed for metrics here, but useful for model insights)\n",
    "y_pred_train = clf.predict(X=X_train)\n",
    "y_pred_prob_train = clf.predict_prob(X=X_train)\n",
    "\n",
    "# Compute training loss (if needed for insights)\n",
    "logisticLoss_train = clf.compute_logisticLoss(X=X_train, y=y_train)\n",
    "\n",
    "# Predictions for test data\n",
    "y_pred_test = clf.predict(X=X_test)\n",
    "y_pred_prob_test = clf.predict_prob(X=X_test)\n",
    "\n",
    "# Compute test loss (optional)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X=X_test, y=y_test)\n",
    "\n",
    "# Print model evaluation\n",
    "print(\"Test Loss: \", logisticLoss_test)\n",
    "\n",
    "# Compute accuracy, precision, and recall on test set\n",
    "accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test, average='binary', pos_label=1)\n",
    "recall = recall_score(y_test, y_pred_test, average='binary', pos_label=1)\n",
    "\n",
    "print(\"Accuracy on Test Set: \", accuracy)\n",
    "print(\"Precision on Test Set: \", precision)\n",
    "print(\"Recall on Test Set: \", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.    RBFOX1_-1     -3 point(s) |   ...\n",
      "2.     RBFOX1_1      3 point(s) | + ...\n",
      "3.    IL17RE_-1     -4 point(s) | + ...\n",
      "4.     GATA3_-1     -2 point(s) | + ...\n",
      "5.       IHO1_0      2 point(s) | + ...\n",
      "6.    MIR4732_2     -5 point(s) | + ...\n",
      "7.      LCOR_-1      4 point(s) | + ...\n",
      "8.     PTF1A_-1      2 point(s) | + ...\n",
      "9.      ITGB1_2      5 point(s) | + ...\n",
      "10.    MEGF10_-1      2 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.2% |   0.3% |   0.4% |   0.7% |   1.1% |   1.7% |   2.6% |   4.0% |   6.2% |\n",
      "SCORE |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  18.0  |\n",
      "RISK  |   9.4% |  14.0% |  20.4% |  28.8% |  38.8% |  50.0% |  61.2% |  71.2% |  79.6% |  86.0% |  90.6% |  93.8% |  96.0% |  97.4% |  98.9% |\n",
      "Training Loss:  1532.0687502892756\n",
      "Test Loss:  446.131283291956\n",
      "Accuracy:  0.91956906729634 \n",
      "AUC:  0.861627255546714\n",
      "Accuracy:  0.8605388272583201 \n",
      "AUC:  0.8628933684145104\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[3],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,4,5,6,7,8,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[1],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,2,3,4,5,6,7,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.    NKIRAS1_0      5 point(s) |   ...\n",
      "2. CDKN2A-DT_-2      3 point(s) | + ...\n",
      "3.   MIR378C_-1      4 point(s) | + ...\n",
      "4.    ERICH6_-1      3 point(s) | + ...\n",
      "5.       SIL1_0      2 point(s) | + ...\n",
      "6.      NICN1_1      5 point(s) | + ...\n",
      "7.     EDEM1_-1     -3 point(s) | + ...\n",
      "8.      APOD_-1      3 point(s) | + ...\n",
      "9.     PLXDC1_1     -5 point(s) | + ...\n",
      "10.         SI_1      3 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -8.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.3% |   0.5% |   1.0% |   2.0% |   3.7% |   6.9% |  12.4% |\n",
      "SCORE |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  17.0  |  18.0  |  19.0  |  20.0  |  21.0  |  22.0  |  23.0  |  24.0  |  25.0  |  26.0  |  28.0  |\n",
      "RISK  |  21.4% |  34.3% |  50.0% |  65.7% |  78.6% |  87.6% |  93.1% |  96.3% |  98.0% |  99.0% |  99.5% |  99.7% |  99.9% |  99.9% | 100.0% | 100.0% | 100.0% |\n",
      "Training Loss:  742.0367316585562\n",
      "Test Loss:  69.91383453145895\n",
      "Accuracy:  0.9654663518299882 \n",
      "AUC:  0.9627408227079385\n",
      "Accuracy:  0.9857369255150554 \n",
      "AUC:  0.9700162074554295\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[4],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,5,6,7,8,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[2],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,3,4,5,6,7,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.    YTHDC2_-1      2 point(s) |   ...\n",
      "2.     NEK10_-1      2 point(s) | + ...\n",
      "3.     SRCIN1_2     -5 point(s) | + ...\n",
      "4.     METTL6_1     -4 point(s) | + ...\n",
      "5.       MBL2_1      2 point(s) | + ...\n",
      "6.    PCDHB12_2      4 point(s) | + ...\n",
      "7.    POU4F3_-1      1 point(s) | + ...\n",
      "8.   PCDHGA5_-2      5 point(s) | + ...\n",
      "9.   SNORD69_-1      1 point(s) | + ...\n",
      "10.     ACSL5_-1     -2 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.3% |   0.5% |   0.9% |   1.6% |   2.8% |   5.0% |\n",
      "SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  17.0  |\n",
      "RISK  |   8.6% |  14.6% |  23.5% |  35.7% |  50.0% |  64.3% |  76.5% |  85.4% |  91.4% |  95.0% |  97.2% |  98.4% |  99.1% |  99.5% |\n",
      "Training Loss:  1268.189997426982\n",
      "Test Loss:  361.46888821878275\n",
      "Accuracy:  0.9303423848878394 \n",
      "AUC:  0.9118633922521471\n",
      "Accuracy:  0.8716323296354992 \n",
      "AUC:  0.9002178796046719\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[5],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,6,7,8,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[6],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,7,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.     NLGN1_-1     -4 point(s) |   ...\n",
      "2.    RHBDL3_-1      2 point(s) | + ...\n",
      "3.     IQCF2_-1     -2 point(s) | + ...\n",
      "4.    THNSL1_-1     -3 point(s) | + ...\n",
      "5.       PURA_0     -3 point(s) | + ...\n",
      "6.   RPL22L1_-2      5 point(s) | + ...\n",
      "7.   RPL22L1_-1      3 point(s) | + ...\n",
      "8.     PHF12_-1      3 point(s) | + ...\n",
      "9.   SLC12A2_-1      3 point(s) | + ...\n",
      "10.      CHSY3_0     -2 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.2% |   0.4% |   0.7% |   1.3% |   2.4% |   4.3% |\n",
      "SCORE |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  16.0  |\n",
      "RISK  |   7.8% |  13.5% |  22.5% |  35.0% |  50.0% |  65.0% |  77.5% |  86.5% |  92.2% |  95.7% |  97.6% |  98.7% |  99.3% |  99.8% |\n",
      "Training Loss:  915.759501656592\n",
      "Test Loss:  99.58753425542326\n",
      "Accuracy:  0.9445100354191264 \n",
      "AUC:  0.9612498039030747\n",
      "Accuracy:  0.9778129952456418 \n",
      "AUC:  0.9713892961876833\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[6],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,7,8,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[7],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,6,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1. CDKN2A-DT_-1     -1 point(s) |   ...\n",
      "2.    RBFOX1_-2      4 point(s) | + ...\n",
      "3.    CDKN2A_-2     -4 point(s) | + ...\n",
      "4.    AKR1C6P_0      2 point(s) | + ...\n",
      "5.      ALG3_-1     -2 point(s) | + ...\n",
      "6.      NFAT5_1      3 point(s) | + ...\n",
      "7.      NFAT5_2      5 point(s) | + ...\n",
      "8.      LCOR_-1     -2 point(s) | + ...\n",
      "9.     EDEM1_-1      2 point(s) | + ...\n",
      "10.    PTPN23_-1     -1 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.7% |   1.8% |   4.7% |  11.9% |  26.9% |\n",
      "SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |\n",
      "RISK  |  50.0% |  73.1% |  88.1% |  95.3% |  98.2% |  99.3% |  99.8% |  99.9% | 100.0% | 100.0% | 100.0% | 100.0% | 100.0% |\n",
      "Training Loss:  1081.9888367568756\n",
      "Test Loss:  271.1745255382249\n",
      "Accuracy:  0.9446576151121606 \n",
      "AUC:  0.9361813526892896\n",
      "Accuracy:  0.919175911251981 \n",
      "AUC:  0.9213019823313942\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[7],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,8,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[9],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,8]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.       PIGW_1      2 point(s) |   ...\n",
      "2.      AGTR1_1     -2 point(s) | + ...\n",
      "3.     PRKCQ_-1     -4 point(s) | + ...\n",
      "4.  B3GALNT1_-1      3 point(s) | + ...\n",
      "5.      ALG3_-2      4 point(s) | + ...\n",
      "6.      ALG3_-1      3 point(s) | + ...\n",
      "7.       TNS4_2     -4 point(s) | + ...\n",
      "8.     CHSY3_-1     -2 point(s) | + ...\n",
      "9.        SI_-1      3 point(s) | + ...\n",
      "10.     TPGS1_-1     -2 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -14.0  |  -12.0  |  -11.0  |  -10.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.1% |   0.1% |   0.1% |   0.2% |   0.4% |   0.6% |   0.9% |   1.5% |   2.4% |   3.7% |   5.8% |\n",
      "SCORE |   1.0  |   2.0  |   3.0  |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  15.0  |\n",
      "RISK  |   8.9% |  13.4% |  19.8% |  28.3% |  38.6% |  50.0% |  61.4% |  71.7% |  80.2% |  86.6% |  91.1% |  94.2% |  96.3% |  98.5% |\n",
      "Training Loss:  1674.2931872335616\n",
      "Test Loss:  381.0453722938337\n",
      "Accuracy:  0.91086186540732 \n",
      "AUC:  0.8263336250499111\n",
      "Accuracy:  0.9057052297939778 \n",
      "AUC:  0.6925313776999417\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[8],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,9]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[5],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,4,6,7,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Risk Score is:\n",
      "1.    CDKN2A_-2     -2 point(s) |   ...\n",
      "2.      NLGN1_0      2 point(s) | + ...\n",
      "3.    FANCD2_-2      5 point(s) | + ...\n",
      "4.    RPUSD3_-1      4 point(s) | + ...\n",
      "5.      MIR31_2      5 point(s) | + ...\n",
      "6.     DCTN4_-1     -3 point(s) | + ...\n",
      "7.     PLXDC1_1     -2 point(s) | + ...\n",
      "8.     PCDH20_1      2 point(s) | + ...\n",
      "9.    ZNF287_-1     -2 point(s) | + ...\n",
      "10.        SI_-1     -2 point(s) | + ...\n",
      "                          SCORE | =    \n",
      "SCORE |  -11.0  |  -9.0  |  -8.0  |  -7.0  |  -6.0  |  -5.0  |  -4.0  |  -3.0  |  -2.0  |  -1.0  |   0.0  |   1.0  |   2.0  |   3.0  |\n",
      "RISK  |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.0% |   0.1% |   0.2% |   0.6% |   1.6% |   4.4% |  11.4% |\n",
      "SCORE |   4.0  |   5.0  |   6.0  |   7.0  |   8.0  |   9.0  |  10.0  |  11.0  |  12.0  |  13.0  |  14.0  |  15.0  |  16.0  |  18.0  |\n",
      "RISK  |  26.4% |  50.0% |  73.6% |  88.6% |  95.6% |  98.4% |  99.4% |  99.8% |  99.9% | 100.0% | 100.0% | 100.0% | 100.0% | 100.0% |\n",
      "Training Loss:  734.7188126546931\n",
      "Test Loss:  85.75801738769364\n",
      "Accuracy:  0.9654663518299882 \n",
      "AUC:  0.9654932654711335\n",
      "Accuracy:  0.9801901743264659 \n",
      "AUC:  0.976268951878708\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"cup.CSV\")\n",
    "primary = df['Primary']\n",
    "df = df.iloc[:,:-1]\n",
    "df= pd.get_dummies(df,columns = df.columns)\n",
    "df = pd.concat([df,primary],axis=1)\n",
    "df.replace(df.iloc[:,-1].unique()[9],1,inplace=True)\n",
    "df.replace(df.iloc[:,-1].unique()[[0,1,2,3,4,5,6,7,8]],-1,inplace=True)\n",
    "\n",
    "df2 = pd.read_csv(\"cup2.csv\")\n",
    "primary2 = df2['Primary']\n",
    "df2 = df2.iloc[:,:-1]\n",
    "df2= pd.get_dummies(df2,columns = df2.columns)\n",
    "df2 = pd.concat([df2,primary2],axis=1)\n",
    "df2.replace(df2.iloc[:,-1].unique()[4],1,inplace=True)\n",
    "df2.replace(df2.iloc[:,-1].unique()[[0,1,2,3,5,6,7,8,9]],-1,inplace=True)\n",
    "empty = pd.DataFrame(columns = h)\n",
    "df2 = pd.concat([empty, df2], axis=0, join='outer')\n",
    "df2 = df2[df2.columns.intersection(h)]\n",
    "df2.fillna(0, inplace=True)\n",
    "\n",
    "X_train = df.iloc[:,:-1].to_numpy()\n",
    "y_train = df['Primary'].to_numpy()\n",
    "\n",
    "X_test = df2.iloc[:,:-1].to_numpy()\n",
    "y_test = df2['Primary'].to_numpy()\n",
    "\n",
    "from fasterrisk.fasterrisk import RiskScoreOptimizer, RiskScoreClassifier\n",
    "\n",
    "sparsity = 10 # produce a risk score model with 5 nonzero coefficients\n",
    "\n",
    "# initialize a risk score optimizer\n",
    "m = RiskScoreOptimizer(X = X_train, y = y_train, k = sparsity)\n",
    "\n",
    "# perform optimization\n",
    "m.optimize()\n",
    "\n",
    "# get all top m solutions from the final diverse pool\n",
    "arr_multiplier, arr_intercept, arr_coefficients = m.get_models() # get m solutions from the diverse pool; Specifically, arr_multiplier.shape=(m, ), arr_intercept.shape=(m, ), arr_coefficients.shape=(m, p)\n",
    "\n",
    "# get the first solution from the final diverse pool by passing an optional model_index; models are ranked in order of increasing logistic loss\n",
    "multiplier, intercept, coefficients = m.get_models(model_index = 0) # get the first solution (smallest logistic loss) from the diverse pool; Specifically, multiplier.shape=(1, ), intercept.shape=(1, ), coefficients.shape=(p, )\n",
    "X_featureNames = df.iloc[:,:-1].columns.values.tolist()# X_featureNames is a list of strings, each of which is the feature name\n",
    "\n",
    "# create a classifier\n",
    "clf = RiskScoreClassifier(multiplier = multiplier, intercept = intercept, coefficients = coefficients, featureNames = X_featureNames)\n",
    "\n",
    "# get the predicted label\n",
    "y_pred = clf.predict(X = X_train)\n",
    "\n",
    "# get the probability of predicting y[i] with label +1\n",
    "y_pred_prob = clf.predict_prob(X = X_train)\n",
    "\n",
    "# compute the logistic loss\n",
    "logisticLoss_train = clf.compute_logisticLoss(X = X_train, y = y_train)\n",
    "logisticLoss_test = clf.compute_logisticLoss(X = X_test, y = y_test)\n",
    "# print the risk score model card\n",
    "clf.print_model_card()\n",
    "\n",
    "# get accuracy and area under the ROC curve (AUC)\n",
    "print(\"Training Loss: \",logisticLoss_train)\n",
    "print(\"Test Loss: \",logisticLoss_test)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_train, y = y_train)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)\n",
    "\n",
    "acc_train, auc_train = clf.get_acc_and_auc(X = X_test, y = y_test)\n",
    "print(\"Accuracy: \" , acc_train, \"\\nAUC: \",auc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasterrisk",
   "language": "python",
   "name": "fasterrisk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
